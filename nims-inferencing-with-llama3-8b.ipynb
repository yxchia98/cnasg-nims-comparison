{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-11-06T08:05:59.547632Z",
     "iopub.status.busy": "2024-11-06T08:05:59.547253Z",
     "iopub.status.idle": "2024-11-06T08:06:03.086465Z",
     "shell.execute_reply": "2024-11-06T08:06:03.085491Z",
     "shell.execute_reply.started": "2024-11-06T08:05:59.547586Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pytorch-ignite                           0.5.1\n",
      "pytorch-lightning                        2.4.0\n",
      "torch                                    2.4.0\n",
      "torchaudio                               2.4.0\n",
      "torchinfo                                1.8.0\n",
      "torchmetrics                             1.4.2\n",
      "torchvision                              0.19.0\n"
     ]
    }
   ],
   "source": [
    "!pip list | grep torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-06T08:06:03.088672Z",
     "iopub.status.busy": "2024-11-06T08:06:03.088346Z",
     "iopub.status.idle": "2024-11-06T08:06:31.644724Z",
     "shell.execute_reply": "2024-11-06T08:06:31.643510Z",
     "shell.execute_reply.started": "2024-11-06T08:06:03.088638Z"
    },
    "scrolled": true,
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting llama-index-llms-huggingface\n",
      "  Downloading llama_index_llms_huggingface-0.3.5-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting huggingface-hub<0.24.0,>=0.23.0 (from llama-index-llms-huggingface)\n",
      "  Downloading huggingface_hub-0.23.5-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting llama-index-core<0.12.0,>=0.11.0 (from llama-index-llms-huggingface)\n",
      "  Downloading llama_index_core-0.11.22-py3-none-any.whl.metadata (2.4 kB)\n",
      "Collecting text-generation<0.8.0,>=0.7.0 (from llama-index-llms-huggingface)\n",
      "  Downloading text_generation-0.7.0-py3-none-any.whl.metadata (8.5 kB)\n",
      "Requirement already satisfied: torch<3.0.0,>=2.1.2 in /opt/conda/lib/python3.10/site-packages (from llama-index-llms-huggingface) (2.4.0)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.37.0 in /opt/conda/lib/python3.10/site-packages (from transformers[torch]<5.0.0,>=4.37.0->llama-index-llms-huggingface) (4.45.1)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<0.24.0,>=0.23.0->llama-index-llms-huggingface) (3.15.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<0.24.0,>=0.23.0->llama-index-llms-huggingface) (2024.6.1)\n",
      "Requirement already satisfied: packaging>=20.9 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<0.24.0,>=0.23.0->llama-index-llms-huggingface) (21.3)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<0.24.0,>=0.23.0->llama-index-llms-huggingface) (6.0.2)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<0.24.0,>=0.23.0->llama-index-llms-huggingface) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<0.24.0,>=0.23.0->llama-index-llms-huggingface) (4.66.4)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<0.24.0,>=0.23.0->llama-index-llms-huggingface) (4.12.2)\n",
      "Requirement already satisfied: SQLAlchemy>=1.4.49 in /opt/conda/lib/python3.10/site-packages (from SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.12.0,>=0.11.0->llama-index-llms-huggingface) (2.0.30)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.6 in /opt/conda/lib/python3.10/site-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-llms-huggingface) (3.9.5)\n",
      "Requirement already satisfied: dataclasses-json in /opt/conda/lib/python3.10/site-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-llms-huggingface) (0.6.7)\n",
      "Requirement already satisfied: deprecated>=1.2.9.3 in /opt/conda/lib/python3.10/site-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-llms-huggingface) (1.2.14)\n",
      "Collecting dirtyjson<2.0.0,>=1.0.8 (from llama-index-core<0.12.0,>=0.11.0->llama-index-llms-huggingface)\n",
      "  Downloading dirtyjson-1.0.8-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: httpx in /opt/conda/lib/python3.10/site-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-llms-huggingface) (0.27.0)\n",
      "Requirement already satisfied: nest-asyncio<2.0.0,>=1.5.8 in /opt/conda/lib/python3.10/site-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-llms-huggingface) (1.6.0)\n",
      "Requirement already satisfied: networkx>=3.0 in /opt/conda/lib/python3.10/site-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-llms-huggingface) (3.3)\n",
      "Collecting nltk>3.8.1 (from llama-index-core<0.12.0,>=0.11.0->llama-index-llms-huggingface)\n",
      "  Downloading nltk-3.9.1-py3-none-any.whl.metadata (2.9 kB)\n",
      "Requirement already satisfied: numpy<2.0.0 in /opt/conda/lib/python3.10/site-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-llms-huggingface) (1.26.4)\n",
      "Requirement already satisfied: pillow>=9.0.0 in /opt/conda/lib/python3.10/site-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-llms-huggingface) (10.3.0)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.0 in /opt/conda/lib/python3.10/site-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-llms-huggingface) (2.9.2)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.2.0 in /opt/conda/lib/python3.10/site-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-llms-huggingface) (8.3.0)\n",
      "Collecting tiktoken>=0.3.3 (from llama-index-core<0.12.0,>=0.11.0->llama-index-llms-huggingface)\n",
      "  Downloading tiktoken-0.8.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
      "Requirement already satisfied: typing-inspect>=0.8.0 in /opt/conda/lib/python3.10/site-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-llms-huggingface) (0.9.0)\n",
      "Requirement already satisfied: wrapt in /opt/conda/lib/python3.10/site-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-llms-huggingface) (1.16.0)\n",
      "Requirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch<3.0.0,>=2.1.2->llama-index-llms-huggingface) (1.13.3)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch<3.0.0,>=2.1.2->llama-index-llms-huggingface) (3.1.4)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers<5.0.0,>=4.37.0->transformers[torch]<5.0.0,>=4.37.0->llama-index-llms-huggingface) (2024.5.15)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers<5.0.0,>=4.37.0->transformers[torch]<5.0.0,>=4.37.0->llama-index-llms-huggingface) (0.4.5)\n",
      "Requirement already satisfied: tokenizers<0.21,>=0.20 in /opt/conda/lib/python3.10/site-packages (from transformers<5.0.0,>=4.37.0->transformers[torch]<5.0.0,>=4.37.0->llama-index-llms-huggingface) (0.20.0)\n",
      "Requirement already satisfied: accelerate>=0.26.0 in /opt/conda/lib/python3.10/site-packages (from transformers[torch]<5.0.0,>=4.37.0->llama-index-llms-huggingface) (0.34.2)\n",
      "Requirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from accelerate>=0.26.0->transformers[torch]<5.0.0,>=4.37.0->llama-index-llms-huggingface) (5.9.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.12.0,>=0.11.0->llama-index-llms-huggingface) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.12.0,>=0.11.0->llama-index-llms-huggingface) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.12.0,>=0.11.0->llama-index-llms-huggingface) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.12.0,>=0.11.0->llama-index-llms-huggingface) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.12.0,>=0.11.0->llama-index-llms-huggingface) (1.9.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.12.0,>=0.11.0->llama-index-llms-huggingface) (4.0.3)\n",
      "Requirement already satisfied: click in /opt/conda/lib/python3.10/site-packages (from nltk>3.8.1->llama-index-core<0.12.0,>=0.11.0->llama-index-llms-huggingface) (8.1.7)\n",
      "Requirement already satisfied: joblib in /opt/conda/lib/python3.10/site-packages (from nltk>3.8.1->llama-index-core<0.12.0,>=0.11.0->llama-index-llms-huggingface) (1.4.2)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.9->huggingface-hub<0.24.0,>=0.23.0->llama-index-llms-huggingface) (3.1.2)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /opt/conda/lib/python3.10/site-packages (from pydantic<3.0.0,>=2.7.0->llama-index-core<0.12.0,>=0.11.0->llama-index-llms-huggingface) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.23.4 in /opt/conda/lib/python3.10/site-packages (from pydantic<3.0.0,>=2.7.0->llama-index-core<0.12.0,>=0.11.0->llama-index-llms-huggingface) (2.23.4)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub<0.24.0,>=0.23.0->llama-index-llms-huggingface) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub<0.24.0,>=0.23.0->llama-index-llms-huggingface) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub<0.24.0,>=0.23.0->llama-index-llms-huggingface) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub<0.24.0,>=0.23.0->llama-index-llms-huggingface) (2024.8.30)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /opt/conda/lib/python3.10/site-packages (from SQLAlchemy>=1.4.49->SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.12.0,>=0.11.0->llama-index-llms-huggingface) (3.0.3)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from typing-inspect>=0.8.0->llama-index-core<0.12.0,>=0.11.0->llama-index-llms-huggingface) (1.0.0)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /opt/conda/lib/python3.10/site-packages (from dataclasses-json->llama-index-core<0.12.0,>=0.11.0->llama-index-llms-huggingface) (3.22.0)\n",
      "Requirement already satisfied: anyio in /opt/conda/lib/python3.10/site-packages (from httpx->llama-index-core<0.12.0,>=0.11.0->llama-index-llms-huggingface) (4.4.0)\n",
      "Requirement already satisfied: httpcore==1.* in /opt/conda/lib/python3.10/site-packages (from httpx->llama-index-core<0.12.0,>=0.11.0->llama-index-llms-huggingface) (1.0.5)\n",
      "Requirement already satisfied: sniffio in /opt/conda/lib/python3.10/site-packages (from httpx->llama-index-core<0.12.0,>=0.11.0->llama-index-llms-huggingface) (1.3.1)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /opt/conda/lib/python3.10/site-packages (from httpcore==1.*->httpx->llama-index-core<0.12.0,>=0.11.0->llama-index-llms-huggingface) (0.14.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch<3.0.0,>=2.1.2->llama-index-llms-huggingface) (2.1.5)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch<3.0.0,>=2.1.2->llama-index-llms-huggingface) (1.3.0)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /opt/conda/lib/python3.10/site-packages (from anyio->httpx->llama-index-core<0.12.0,>=0.11.0->llama-index-llms-huggingface) (1.2.0)\n",
      "Downloading llama_index_llms_huggingface-0.3.5-py3-none-any.whl (11 kB)\n",
      "Downloading huggingface_hub-0.23.5-py3-none-any.whl (402 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m402.8/402.8 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading llama_index_core-0.11.22-py3-none-any.whl (1.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m35.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading text_generation-0.7.0-py3-none-any.whl (12 kB)\n",
      "Downloading dirtyjson-1.0.8-py3-none-any.whl (25 kB)\n",
      "Downloading nltk-3.9.1-py3-none-any.whl (1.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m53.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tiktoken-0.8.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m45.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: dirtyjson, nltk, tiktoken, huggingface-hub, text-generation, llama-index-core, llama-index-llms-huggingface\n",
      "  Attempting uninstall: nltk\n",
      "    Found existing installation: nltk 3.2.4\n",
      "    Uninstalling nltk-3.2.4:\n",
      "      Successfully uninstalled nltk-3.2.4\n",
      "  Attempting uninstall: huggingface-hub\n",
      "    Found existing installation: huggingface-hub 0.25.1\n",
      "    Uninstalling huggingface-hub-0.25.1:\n",
      "      Successfully uninstalled huggingface-hub-0.25.1\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "preprocessing 0.1.13 requires nltk==3.2.4, but you have nltk 3.9.1 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed dirtyjson-1.0.8 huggingface-hub-0.23.5 llama-index-core-0.11.22 llama-index-llms-huggingface-0.3.5 nltk-3.9.1 text-generation-0.7.0 tiktoken-0.8.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Collecting llama-index-llms-huggingface-api\n",
      "  Downloading llama_index_llms_huggingface_api-0.2.0-py3-none-any.whl.metadata (1.3 kB)\n",
      "Requirement already satisfied: huggingface-hub<0.24.0,>=0.23.0 in /opt/conda/lib/python3.10/site-packages (from llama-index-llms-huggingface-api) (0.23.5)\n",
      "Requirement already satisfied: llama-index-core<0.12.0,>=0.11.0 in /opt/conda/lib/python3.10/site-packages (from llama-index-llms-huggingface-api) (0.11.22)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<0.24.0,>=0.23.0->llama-index-llms-huggingface-api) (3.15.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<0.24.0,>=0.23.0->llama-index-llms-huggingface-api) (2024.6.1)\n",
      "Requirement already satisfied: packaging>=20.9 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<0.24.0,>=0.23.0->llama-index-llms-huggingface-api) (21.3)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<0.24.0,>=0.23.0->llama-index-llms-huggingface-api) (6.0.2)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<0.24.0,>=0.23.0->llama-index-llms-huggingface-api) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<0.24.0,>=0.23.0->llama-index-llms-huggingface-api) (4.66.4)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<0.24.0,>=0.23.0->llama-index-llms-huggingface-api) (4.12.2)\n",
      "Requirement already satisfied: SQLAlchemy>=1.4.49 in /opt/conda/lib/python3.10/site-packages (from SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.12.0,>=0.11.0->llama-index-llms-huggingface-api) (2.0.30)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.6 in /opt/conda/lib/python3.10/site-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-llms-huggingface-api) (3.9.5)\n",
      "Requirement already satisfied: dataclasses-json in /opt/conda/lib/python3.10/site-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-llms-huggingface-api) (0.6.7)\n",
      "Requirement already satisfied: deprecated>=1.2.9.3 in /opt/conda/lib/python3.10/site-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-llms-huggingface-api) (1.2.14)\n",
      "Requirement already satisfied: dirtyjson<2.0.0,>=1.0.8 in /opt/conda/lib/python3.10/site-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-llms-huggingface-api) (1.0.8)\n",
      "Requirement already satisfied: httpx in /opt/conda/lib/python3.10/site-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-llms-huggingface-api) (0.27.0)\n",
      "Requirement already satisfied: nest-asyncio<2.0.0,>=1.5.8 in /opt/conda/lib/python3.10/site-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-llms-huggingface-api) (1.6.0)\n",
      "Requirement already satisfied: networkx>=3.0 in /opt/conda/lib/python3.10/site-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-llms-huggingface-api) (3.3)\n",
      "Requirement already satisfied: nltk>3.8.1 in /opt/conda/lib/python3.10/site-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-llms-huggingface-api) (3.9.1)\n",
      "Requirement already satisfied: numpy<2.0.0 in /opt/conda/lib/python3.10/site-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-llms-huggingface-api) (1.26.4)\n",
      "Requirement already satisfied: pillow>=9.0.0 in /opt/conda/lib/python3.10/site-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-llms-huggingface-api) (10.3.0)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.0 in /opt/conda/lib/python3.10/site-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-llms-huggingface-api) (2.9.2)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.2.0 in /opt/conda/lib/python3.10/site-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-llms-huggingface-api) (8.3.0)\n",
      "Requirement already satisfied: tiktoken>=0.3.3 in /opt/conda/lib/python3.10/site-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-llms-huggingface-api) (0.8.0)\n",
      "Requirement already satisfied: typing-inspect>=0.8.0 in /opt/conda/lib/python3.10/site-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-llms-huggingface-api) (0.9.0)\n",
      "Requirement already satisfied: wrapt in /opt/conda/lib/python3.10/site-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-llms-huggingface-api) (1.16.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.12.0,>=0.11.0->llama-index-llms-huggingface-api) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.12.0,>=0.11.0->llama-index-llms-huggingface-api) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.12.0,>=0.11.0->llama-index-llms-huggingface-api) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.12.0,>=0.11.0->llama-index-llms-huggingface-api) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.12.0,>=0.11.0->llama-index-llms-huggingface-api) (1.9.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.12.0,>=0.11.0->llama-index-llms-huggingface-api) (4.0.3)\n",
      "Requirement already satisfied: click in /opt/conda/lib/python3.10/site-packages (from nltk>3.8.1->llama-index-core<0.12.0,>=0.11.0->llama-index-llms-huggingface-api) (8.1.7)\n",
      "Requirement already satisfied: joblib in /opt/conda/lib/python3.10/site-packages (from nltk>3.8.1->llama-index-core<0.12.0,>=0.11.0->llama-index-llms-huggingface-api) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /opt/conda/lib/python3.10/site-packages (from nltk>3.8.1->llama-index-core<0.12.0,>=0.11.0->llama-index-llms-huggingface-api) (2024.5.15)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.9->huggingface-hub<0.24.0,>=0.23.0->llama-index-llms-huggingface-api) (3.1.2)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /opt/conda/lib/python3.10/site-packages (from pydantic<3.0.0,>=2.7.0->llama-index-core<0.12.0,>=0.11.0->llama-index-llms-huggingface-api) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.23.4 in /opt/conda/lib/python3.10/site-packages (from pydantic<3.0.0,>=2.7.0->llama-index-core<0.12.0,>=0.11.0->llama-index-llms-huggingface-api) (2.23.4)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub<0.24.0,>=0.23.0->llama-index-llms-huggingface-api) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub<0.24.0,>=0.23.0->llama-index-llms-huggingface-api) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub<0.24.0,>=0.23.0->llama-index-llms-huggingface-api) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub<0.24.0,>=0.23.0->llama-index-llms-huggingface-api) (2024.8.30)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /opt/conda/lib/python3.10/site-packages (from SQLAlchemy>=1.4.49->SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.12.0,>=0.11.0->llama-index-llms-huggingface-api) (3.0.3)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from typing-inspect>=0.8.0->llama-index-core<0.12.0,>=0.11.0->llama-index-llms-huggingface-api) (1.0.0)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /opt/conda/lib/python3.10/site-packages (from dataclasses-json->llama-index-core<0.12.0,>=0.11.0->llama-index-llms-huggingface-api) (3.22.0)\n",
      "Requirement already satisfied: anyio in /opt/conda/lib/python3.10/site-packages (from httpx->llama-index-core<0.12.0,>=0.11.0->llama-index-llms-huggingface-api) (4.4.0)\n",
      "Requirement already satisfied: httpcore==1.* in /opt/conda/lib/python3.10/site-packages (from httpx->llama-index-core<0.12.0,>=0.11.0->llama-index-llms-huggingface-api) (1.0.5)\n",
      "Requirement already satisfied: sniffio in /opt/conda/lib/python3.10/site-packages (from httpx->llama-index-core<0.12.0,>=0.11.0->llama-index-llms-huggingface-api) (1.3.1)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /opt/conda/lib/python3.10/site-packages (from httpcore==1.*->httpx->llama-index-core<0.12.0,>=0.11.0->llama-index-llms-huggingface-api) (0.14.0)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /opt/conda/lib/python3.10/site-packages (from anyio->httpx->llama-index-core<0.12.0,>=0.11.0->llama-index-llms-huggingface-api) (1.2.0)\n",
      "Downloading llama_index_llms_huggingface_api-0.2.0-py3-none-any.whl (5.0 kB)\n",
      "Installing collected packages: llama-index-llms-huggingface-api\n",
      "Successfully installed llama-index-llms-huggingface-api-0.2.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install llama-index-llms-huggingface\n",
    "%pip install llama-index-llms-huggingface-api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-06T08:06:31.646474Z",
     "iopub.status.busy": "2024-11-06T08:06:31.646135Z",
     "iopub.status.idle": "2024-11-06T08:06:43.560521Z",
     "shell.execute_reply": "2024-11-06T08:06:43.559407Z",
     "shell.execute_reply.started": "2024-11-06T08:06:31.646438Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers[torch] in /opt/conda/lib/python3.10/site-packages (4.45.1)\n",
      "Requirement already satisfied: huggingface_hub[inference] in /opt/conda/lib/python3.10/site-packages (0.23.5)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers[torch]) (3.15.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers[torch]) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers[torch]) (21.3)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers[torch]) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers[torch]) (2024.5.15)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers[torch]) (2.32.3)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers[torch]) (0.4.5)\n",
      "Requirement already satisfied: tokenizers<0.21,>=0.20 in /opt/conda/lib/python3.10/site-packages (from transformers[torch]) (0.20.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers[torch]) (4.66.4)\n",
      "Requirement already satisfied: accelerate>=0.26.0 in /opt/conda/lib/python3.10/site-packages (from transformers[torch]) (0.34.2)\n",
      "Requirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (from transformers[torch]) (2.4.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface_hub[inference]) (2024.6.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface_hub[inference]) (4.12.2)\n",
      "Requirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from huggingface_hub[inference]) (3.9.5)\n",
      "Collecting minijinja>=1.0 (from huggingface_hub[inference])\n",
      "  Downloading minijinja-2.2.0-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (8.8 kB)\n",
      "Requirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from accelerate>=0.26.0->transformers[torch]) (5.9.3)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->transformers[torch]) (3.1.2)\n",
      "Requirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch->transformers[torch]) (1.13.3)\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch->transformers[torch]) (3.3)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch->transformers[torch]) (3.1.4)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->huggingface_hub[inference]) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->huggingface_hub[inference]) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->huggingface_hub[inference]) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->huggingface_hub[inference]) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->huggingface_hub[inference]) (1.9.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->huggingface_hub[inference]) (4.0.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers[torch]) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers[torch]) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers[torch]) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers[torch]) (2024.8.30)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch->transformers[torch]) (2.1.5)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch->transformers[torch]) (1.3.0)\n",
      "Downloading minijinja-2.2.0-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (861 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m861.9/861.9 kB\u001b[0m \u001b[31m11.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: minijinja\n",
      "Successfully installed minijinja-2.2.0\n"
     ]
    }
   ],
   "source": [
    "!pip install \"transformers[torch]\" \"huggingface_hub[inference]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-06T08:06:43.565694Z",
     "iopub.status.busy": "2024-11-06T08:06:43.565042Z",
     "iopub.status.idle": "2024-11-06T08:07:01.145303Z",
     "shell.execute_reply": "2024-11-06T08:07:01.144340Z",
     "shell.execute_reply.started": "2024-11-06T08:06:43.565655Z"
    },
    "scrolled": true,
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting llama-index\n",
      "  Downloading llama_index-0.11.22-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting llama-index-agent-openai<0.4.0,>=0.3.4 (from llama-index)\n",
      "  Downloading llama_index_agent_openai-0.3.4-py3-none-any.whl.metadata (728 bytes)\n",
      "Collecting llama-index-cli<0.4.0,>=0.3.1 (from llama-index)\n",
      "  Downloading llama_index_cli-0.3.1-py3-none-any.whl.metadata (1.5 kB)\n",
      "Requirement already satisfied: llama-index-core<0.12.0,>=0.11.22 in /opt/conda/lib/python3.10/site-packages (from llama-index) (0.11.22)\n",
      "Collecting llama-index-embeddings-openai<0.3.0,>=0.2.4 (from llama-index)\n",
      "  Downloading llama_index_embeddings_openai-0.2.5-py3-none-any.whl.metadata (686 bytes)\n",
      "Collecting llama-index-indices-managed-llama-cloud>=0.3.0 (from llama-index)\n",
      "  Downloading llama_index_indices_managed_llama_cloud-0.4.0-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting llama-index-legacy<0.10.0,>=0.9.48 (from llama-index)\n",
      "  Downloading llama_index_legacy-0.9.48.post3-py3-none-any.whl.metadata (8.5 kB)\n",
      "Collecting llama-index-llms-openai<0.3.0,>=0.2.10 (from llama-index)\n",
      "  Downloading llama_index_llms_openai-0.2.16-py3-none-any.whl.metadata (3.3 kB)\n",
      "Collecting llama-index-multi-modal-llms-openai<0.3.0,>=0.2.0 (from llama-index)\n",
      "  Downloading llama_index_multi_modal_llms_openai-0.2.3-py3-none-any.whl.metadata (729 bytes)\n",
      "Collecting llama-index-program-openai<0.3.0,>=0.2.0 (from llama-index)\n",
      "  Downloading llama_index_program_openai-0.2.0-py3-none-any.whl.metadata (766 bytes)\n",
      "Collecting llama-index-question-gen-openai<0.3.0,>=0.2.0 (from llama-index)\n",
      "  Downloading llama_index_question_gen_openai-0.2.0-py3-none-any.whl.metadata (785 bytes)\n",
      "Collecting llama-index-readers-file<0.3.0,>=0.2.0 (from llama-index)\n",
      "  Downloading llama_index_readers_file-0.2.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting llama-index-readers-llama-parse>=0.3.0 (from llama-index)\n",
      "  Downloading llama_index_readers_llama_parse-0.3.0-py3-none-any.whl.metadata (3.5 kB)\n",
      "Requirement already satisfied: nltk>3.8.1 in /opt/conda/lib/python3.10/site-packages (from llama-index) (3.9.1)\n",
      "Collecting openai>=1.14.0 (from llama-index-agent-openai<0.4.0,>=0.3.4->llama-index)\n",
      "  Downloading openai-1.54.1-py3-none-any.whl.metadata (24 kB)\n",
      "Requirement already satisfied: PyYAML>=6.0.1 in /opt/conda/lib/python3.10/site-packages (from llama-index-core<0.12.0,>=0.11.22->llama-index) (6.0.2)\n",
      "Requirement already satisfied: SQLAlchemy>=1.4.49 in /opt/conda/lib/python3.10/site-packages (from SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.12.0,>=0.11.22->llama-index) (2.0.30)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.6 in /opt/conda/lib/python3.10/site-packages (from llama-index-core<0.12.0,>=0.11.22->llama-index) (3.9.5)\n",
      "Requirement already satisfied: dataclasses-json in /opt/conda/lib/python3.10/site-packages (from llama-index-core<0.12.0,>=0.11.22->llama-index) (0.6.7)\n",
      "Requirement already satisfied: deprecated>=1.2.9.3 in /opt/conda/lib/python3.10/site-packages (from llama-index-core<0.12.0,>=0.11.22->llama-index) (1.2.14)\n",
      "Requirement already satisfied: dirtyjson<2.0.0,>=1.0.8 in /opt/conda/lib/python3.10/site-packages (from llama-index-core<0.12.0,>=0.11.22->llama-index) (1.0.8)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from llama-index-core<0.12.0,>=0.11.22->llama-index) (2024.6.1)\n",
      "Requirement already satisfied: httpx in /opt/conda/lib/python3.10/site-packages (from llama-index-core<0.12.0,>=0.11.22->llama-index) (0.27.0)\n",
      "Requirement already satisfied: nest-asyncio<2.0.0,>=1.5.8 in /opt/conda/lib/python3.10/site-packages (from llama-index-core<0.12.0,>=0.11.22->llama-index) (1.6.0)\n",
      "Requirement already satisfied: networkx>=3.0 in /opt/conda/lib/python3.10/site-packages (from llama-index-core<0.12.0,>=0.11.22->llama-index) (3.3)\n",
      "Requirement already satisfied: numpy<2.0.0 in /opt/conda/lib/python3.10/site-packages (from llama-index-core<0.12.0,>=0.11.22->llama-index) (1.26.4)\n",
      "Requirement already satisfied: pillow>=9.0.0 in /opt/conda/lib/python3.10/site-packages (from llama-index-core<0.12.0,>=0.11.22->llama-index) (10.3.0)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.0 in /opt/conda/lib/python3.10/site-packages (from llama-index-core<0.12.0,>=0.11.22->llama-index) (2.9.2)\n",
      "Requirement already satisfied: requests>=2.31.0 in /opt/conda/lib/python3.10/site-packages (from llama-index-core<0.12.0,>=0.11.22->llama-index) (2.32.3)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.2.0 in /opt/conda/lib/python3.10/site-packages (from llama-index-core<0.12.0,>=0.11.22->llama-index) (8.3.0)\n",
      "Requirement already satisfied: tiktoken>=0.3.3 in /opt/conda/lib/python3.10/site-packages (from llama-index-core<0.12.0,>=0.11.22->llama-index) (0.8.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.66.1 in /opt/conda/lib/python3.10/site-packages (from llama-index-core<0.12.0,>=0.11.22->llama-index) (4.66.4)\n",
      "Requirement already satisfied: typing-extensions>=4.5.0 in /opt/conda/lib/python3.10/site-packages (from llama-index-core<0.12.0,>=0.11.22->llama-index) (4.12.2)\n",
      "Requirement already satisfied: typing-inspect>=0.8.0 in /opt/conda/lib/python3.10/site-packages (from llama-index-core<0.12.0,>=0.11.22->llama-index) (0.9.0)\n",
      "Requirement already satisfied: wrapt in /opt/conda/lib/python3.10/site-packages (from llama-index-core<0.12.0,>=0.11.22->llama-index) (1.16.0)\n",
      "Collecting llama-cloud>=0.0.11 (from llama-index-indices-managed-llama-cloud>=0.3.0->llama-index)\n",
      "  Downloading llama_cloud-0.1.4-py3-none-any.whl.metadata (814 bytes)\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from llama-index-legacy<0.10.0,>=0.9.48->llama-index) (2.2.2)\n",
      "Requirement already satisfied: beautifulsoup4<5.0.0,>=4.12.3 in /opt/conda/lib/python3.10/site-packages (from llama-index-readers-file<0.3.0,>=0.2.0->llama-index) (4.12.3)\n",
      "Collecting pypdf<5.0.0,>=4.0.1 (from llama-index-readers-file<0.3.0,>=0.2.0->llama-index)\n",
      "  Downloading pypdf-4.3.1-py3-none-any.whl.metadata (7.4 kB)\n",
      "Collecting striprtf<0.0.27,>=0.0.26 (from llama-index-readers-file<0.3.0,>=0.2.0->llama-index)\n",
      "  Downloading striprtf-0.0.26-py3-none-any.whl.metadata (2.1 kB)\n",
      "Collecting llama-parse>=0.5.0 (from llama-index-readers-llama-parse>=0.3.0->llama-index)\n",
      "  Downloading llama_parse-0.5.13-py3-none-any.whl.metadata (6.9 kB)\n",
      "Requirement already satisfied: click in /opt/conda/lib/python3.10/site-packages (from nltk>3.8.1->llama-index) (8.1.7)\n",
      "Requirement already satisfied: joblib in /opt/conda/lib/python3.10/site-packages (from nltk>3.8.1->llama-index) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /opt/conda/lib/python3.10/site-packages (from nltk>3.8.1->llama-index) (2024.5.15)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.12.0,>=0.11.22->llama-index) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.12.0,>=0.11.22->llama-index) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.12.0,>=0.11.22->llama-index) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.12.0,>=0.11.22->llama-index) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.12.0,>=0.11.22->llama-index) (1.9.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.12.0,>=0.11.22->llama-index) (4.0.3)\n",
      "Requirement already satisfied: soupsieve>1.2 in /opt/conda/lib/python3.10/site-packages (from beautifulsoup4<5.0.0,>=4.12.3->llama-index-readers-file<0.3.0,>=0.2.0->llama-index) (2.5)\n",
      "Requirement already satisfied: anyio in /opt/conda/lib/python3.10/site-packages (from httpx->llama-index-core<0.12.0,>=0.11.22->llama-index) (4.4.0)\n",
      "Requirement already satisfied: certifi in /opt/conda/lib/python3.10/site-packages (from httpx->llama-index-core<0.12.0,>=0.11.22->llama-index) (2024.8.30)\n",
      "Requirement already satisfied: httpcore==1.* in /opt/conda/lib/python3.10/site-packages (from httpx->llama-index-core<0.12.0,>=0.11.22->llama-index) (1.0.5)\n",
      "Requirement already satisfied: idna in /opt/conda/lib/python3.10/site-packages (from httpx->llama-index-core<0.12.0,>=0.11.22->llama-index) (3.7)\n",
      "Requirement already satisfied: sniffio in /opt/conda/lib/python3.10/site-packages (from httpx->llama-index-core<0.12.0,>=0.11.22->llama-index) (1.3.1)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /opt/conda/lib/python3.10/site-packages (from httpcore==1.*->httpx->llama-index-core<0.12.0,>=0.11.22->llama-index) (0.14.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /opt/conda/lib/python3.10/site-packages (from openai>=1.14.0->llama-index-agent-openai<0.4.0,>=0.3.4->llama-index) (1.9.0)\n",
      "Collecting jiter<1,>=0.4.0 (from openai>=1.14.0->llama-index-agent-openai<0.4.0,>=0.3.4->llama-index)\n",
      "  Downloading jiter-0.7.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.2 kB)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /opt/conda/lib/python3.10/site-packages (from pydantic<3.0.0,>=2.7.0->llama-index-core<0.12.0,>=0.11.22->llama-index) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.23.4 in /opt/conda/lib/python3.10/site-packages (from pydantic<3.0.0,>=2.7.0->llama-index-core<0.12.0,>=0.11.22->llama-index) (2.23.4)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.31.0->llama-index-core<0.12.0,>=0.11.22->llama-index) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.31.0->llama-index-core<0.12.0,>=0.11.22->llama-index) (1.26.18)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /opt/conda/lib/python3.10/site-packages (from SQLAlchemy>=1.4.49->SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.12.0,>=0.11.22->llama-index) (3.0.3)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from typing-inspect>=0.8.0->llama-index-core<0.12.0,>=0.11.22->llama-index) (1.0.0)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /opt/conda/lib/python3.10/site-packages (from dataclasses-json->llama-index-core<0.12.0,>=0.11.22->llama-index) (3.22.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->llama-index-legacy<0.10.0,>=0.9.48->llama-index) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->llama-index-legacy<0.10.0,>=0.9.48->llama-index) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas->llama-index-legacy<0.10.0,>=0.9.48->llama-index) (2024.1)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /opt/conda/lib/python3.10/site-packages (from anyio->httpx->llama-index-core<0.12.0,>=0.11.22->llama-index) (1.2.0)\n",
      "Requirement already satisfied: packaging>=17.0 in /opt/conda/lib/python3.10/site-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json->llama-index-core<0.12.0,>=0.11.22->llama-index) (21.3)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->llama-index-legacy<0.10.0,>=0.9.48->llama-index) (1.16.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=17.0->marshmallow<4.0.0,>=3.18.0->dataclasses-json->llama-index-core<0.12.0,>=0.11.22->llama-index) (3.1.2)\n",
      "Downloading llama_index-0.11.22-py3-none-any.whl (6.8 kB)\n",
      "Downloading llama_index_agent_openai-0.3.4-py3-none-any.whl (13 kB)\n",
      "Downloading llama_index_cli-0.3.1-py3-none-any.whl (27 kB)\n",
      "Downloading llama_index_embeddings_openai-0.2.5-py3-none-any.whl (6.1 kB)\n",
      "Downloading llama_index_indices_managed_llama_cloud-0.4.0-py3-none-any.whl (10 kB)\n",
      "Downloading llama_index_legacy-0.9.48.post3-py3-none-any.whl (1.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m17.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading llama_index_llms_openai-0.2.16-py3-none-any.whl (13 kB)\n",
      "Downloading llama_index_multi_modal_llms_openai-0.2.3-py3-none-any.whl (5.9 kB)\n",
      "Downloading llama_index_program_openai-0.2.0-py3-none-any.whl (5.3 kB)\n",
      "Downloading llama_index_question_gen_openai-0.2.0-py3-none-any.whl (2.9 kB)\n",
      "Downloading llama_index_readers_file-0.2.2-py3-none-any.whl (38 kB)\n",
      "Downloading llama_index_readers_llama_parse-0.3.0-py3-none-any.whl (2.5 kB)\n",
      "Downloading llama_cloud-0.1.4-py3-none-any.whl (176 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m176.8/176.8 kB\u001b[0m \u001b[31m11.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading llama_parse-0.5.13-py3-none-any.whl (13 kB)\n",
      "Downloading openai-1.54.1-py3-none-any.whl (389 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m389.3/389.3 kB\u001b[0m \u001b[31m25.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pypdf-4.3.1-py3-none-any.whl (295 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m295.8/295.8 kB\u001b[0m \u001b[31m18.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading striprtf-0.0.26-py3-none-any.whl (6.9 kB)\n",
      "Downloading jiter-0.7.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (327 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m327.5/327.5 kB\u001b[0m \u001b[31m21.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: striprtf, pypdf, jiter, openai, llama-cloud, llama-index-legacy, llama-parse, llama-index-readers-file, llama-index-llms-openai, llama-index-indices-managed-llama-cloud, llama-index-embeddings-openai, llama-index-readers-llama-parse, llama-index-multi-modal-llms-openai, llama-index-cli, llama-index-agent-openai, llama-index-program-openai, llama-index-question-gen-openai, llama-index\n",
      "  Attempting uninstall: pypdf\n",
      "    Found existing installation: pypdf 5.0.1\n",
      "    Uninstalling pypdf-5.0.1:\n",
      "      Successfully uninstalled pypdf-5.0.1\n",
      "Successfully installed jiter-0.7.0 llama-cloud-0.1.4 llama-index-0.11.22 llama-index-agent-openai-0.3.4 llama-index-cli-0.3.1 llama-index-embeddings-openai-0.2.5 llama-index-indices-managed-llama-cloud-0.4.0 llama-index-legacy-0.9.48.post3 llama-index-llms-openai-0.2.16 llama-index-multi-modal-llms-openai-0.2.3 llama-index-program-openai-0.2.0 llama-index-question-gen-openai-0.2.0 llama-index-readers-file-0.2.2 llama-index-readers-llama-parse-0.3.0 llama-parse-0.5.13 openai-1.54.1 pypdf-4.3.1 striprtf-0.0.26\n"
     ]
    }
   ],
   "source": [
    "!pip install llama-index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-06T08:07:01.147340Z",
     "iopub.status.busy": "2024-11-06T08:07:01.146907Z",
     "iopub.status.idle": "2024-11-06T08:07:01.152307Z",
     "shell.execute_reply": "2024-11-06T08:07:01.151492Z",
     "shell.execute_reply.started": "2024-11-06T08:07:01.147293Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "hf_token = \"\"\n",
    "\n",
    "import getpass\n",
    "import os\n",
    "\n",
    "# del os.environ['NVIDIA_API_KEY']  ## delete key and reset\n",
    "if os.environ.get(\"HF_TOKEN\", \"\").startswith(\"hf_\"):\n",
    "    print(\"Valid HF_TOKEN already in environment. Delete to reset\")\n",
    "else:\n",
    "    hf_token = getpass.getpass(\"HF TOKEN (starts with hf_): \")\n",
    "    assert hf_token.startswith(\n",
    "        \"hf_\"\n",
    "    ), f\"{hf_token[:5]}... is not a valid key\"\n",
    "    os.environ[\"HF_TOKEN\"] = hf_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-06T08:07:01.153745Z",
     "iopub.status.busy": "2024-11-06T08:07:01.153486Z",
     "iopub.status.idle": "2024-11-06T08:07:10.302693Z",
     "shell.execute_reply": "2024-11-06T08:07:10.301680Z",
     "shell.execute_reply.started": "2024-11-06T08:07:01.153715Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "272693cce5714d11b05197ba22687a5f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/51.0k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "24c5f0979560417a8230c2b6d62d7940",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/9.09M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f5453366a14b439ab4af24cbc297ecf3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/73.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    \"meta-llama/Llama-3.1-8B-Instruct\",\n",
    "    token=os.environ.get(\"HF_TOKEN\", \"\"),\n",
    ")\n",
    "\n",
    "stopping_ids = [\n",
    "    tokenizer.eos_token_id,\n",
    "    tokenizer.convert_tokens_to_ids(\"<|eot_id|>\"),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-06T08:07:10.304310Z",
     "iopub.status.busy": "2024-11-06T08:07:10.303903Z",
     "iopub.status.idle": "2024-11-06T08:14:51.425982Z",
     "shell.execute_reply": "2024-11-06T08:14:51.425136Z",
     "shell.execute_reply.started": "2024-11-06T08:07:10.304278Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/pydantic/_internal/_fields.py:132: UserWarning: Field \"model_id\" in DeployedModel has conflict with protected namespace \"model_\".\n",
      "\n",
      "You may be able to resolve this warning by setting `model_config['protected_namespaces'] = ()`.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/pydantic/_internal/_fields.py:132: UserWarning: Field \"model_name\" in HuggingFaceLLM has conflict with protected namespace \"model_\".\n",
      "\n",
      "You may be able to resolve this warning by setting `model_config['protected_namespaces'] = ()`.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/pydantic/_internal/_fields.py:132: UserWarning: Field \"model_kwargs\" in HuggingFaceLLM has conflict with protected namespace \"model_\".\n",
      "\n",
      "You may be able to resolve this warning by setting `model_config['protected_namespaces'] = ()`.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/pydantic/_internal/_fields.py:132: UserWarning: Field \"model_name\" in HuggingFaceInferenceAPI has conflict with protected namespace \"model_\".\n",
      "\n",
      "You may be able to resolve this warning by setting `model_config['protected_namespaces'] = ()`.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/pydantic/_internal/_fields.py:132: UserWarning: Field \"model_name\" in TextGenerationInference has conflict with protected namespace \"model_\".\n",
      "\n",
      "You may be able to resolve this warning by setting `model_config['protected_namespaces'] = ()`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c5390402c8c4b4cab7705f6924de9dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/654 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "42965f9ad0e9474086a9326d8b5c7316",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/23.9k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8df450e0fe864754990dd3d823b5a1f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "42c78273fbd34dfe8cae948482b874d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00004.safetensors:   0%|          | 0.00/4.98G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aee4007c890e4324b7865d5ae667c8df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00004.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "602f7fce950d4bd19d113a39a1f24f9c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00003-of-00004.safetensors:   0%|          | 0.00/4.92G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8cd25d28592d4d589f35bfbfd96fd508",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00004-of-00004.safetensors:   0%|          | 0.00/1.17G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc7a6fcc87cc43869ee81921c8792ea1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "51207d92e35f49c09b00cc37457b377b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/187 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# generate_kwargs parameters are taken from https://huggingface.co/meta-llama/Llama-3.1-8B-Instruct\n",
    "\n",
    "import torch\n",
    "from llama_index.llms.huggingface import HuggingFaceLLM\n",
    "\n",
    "# Optional quantization to 4bit\n",
    "# import torch\n",
    "# from transformers import BitsAndBytesConfig\n",
    "\n",
    "# quantization_config = BitsAndBytesConfig(\n",
    "#     load_in_4bit=True,\n",
    "#     bnb_4bit_compute_dtype=torch.float16,\n",
    "#     bnb_4bit_quant_type=\"nf4\",\n",
    "#     bnb_4bit_use_double_quant=True,\n",
    "# )\n",
    "\n",
    "local_llm = HuggingFaceLLM(\n",
    "    model_name=\"meta-llama/Llama-3.1-8B-Instruct\",\n",
    "    model_kwargs={\n",
    "        \"token\": hf_token,\n",
    "        \"torch_dtype\": torch.bfloat16,  # comment this line and uncomment below to use 4bit\n",
    "        # \"quantization_config\": quantization_config\n",
    "    },\n",
    "    generate_kwargs={\n",
    "        \"do_sample\": True,\n",
    "        \"temperature\": 0.6,\n",
    "        \"top_p\": 0.9,\n",
    "    },\n",
    "    tokenizer_name=\"meta-llama/Llama-3.1-8B-Instruct\",\n",
    "    tokenizer_kwargs={\"token\": hf_token},\n",
    "    stopping_ids=stopping_ids,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-06T08:14:51.428415Z",
     "iopub.status.busy": "2024-11-06T08:14:51.427320Z",
     "iopub.status.idle": "2024-11-06T08:15:22.335770Z",
     "shell.execute_reply": "2024-11-06T08:15:22.334761Z",
     "shell.execute_reply.started": "2024-11-06T08:14:51.428361Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ", there was a beautiful and vibrant town called Willow Creek. The town was surrounded by lush green forests, rolling hills, and sparkling streams. The residents of Willow Creek lived in harmony with nature, and the town was known for its natural beauty and tranquility.\n",
      "\n",
      "One day, a group of strangers arrived in Willow Creek. They were a diverse group of people, each with their own unique skills and talents. There was a young artist named Lily, who was known for her beautiful paintings. There was a skilled carpenter named Jack, who could build anything from a simple chair to a grand mansion. There was a talented musician named Sarah, who could play any instrument and sing like a bird. And there was a wise and kind old man named George, who had lived in the town for many years and knew its history and secrets.\n",
      "\n",
      "The strangers were welcomed with open arms by the residents of Willow Creek. They were given a warm and cozy home, and they quickly became a part of the community. Lily began to paint the beautiful landscapes of the town, Jack built a beautiful new house for the town's mayor, and Sarah played her music at the town's festivals. George shared his knowledge and wisdom with the townspeople, and soon he was known as the town's elder and leader\n"
     ]
    }
   ],
   "source": [
    "response = local_llm.complete(\"What is Dell Technologies?\")\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-06T08:15:22.337692Z",
     "iopub.status.busy": "2024-11-06T08:15:22.336960Z",
     "iopub.status.idle": "2024-11-06T08:15:40.168241Z",
     "shell.execute_reply": "2024-11-06T08:15:40.167303Z",
     "shell.execute_reply.started": "2024-11-06T08:15:22.337656Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ", there was a young man named Jack who lived in a small village surrounded by a dense forest. Jack was a curious and adventurous soul, always eager to explore the unknown and discover new things. One day, he decided to venture into the forest, determined to find the source of a strange and beautiful melody that he had heard echoing through the trees.\n",
      "\n",
      "As he wandered deeper into the forest, Jack encountered all manner of strange and wondrous creatures, each one more fantastical than the last. He met a wise old owl who offered him sage advice, a mischievous band of fairies who played tricks on him, and even a magnificent unicorn who galloped alongside him for a while.\n",
      "\n",
      "Despite the many wonders he encountered, Jack remained fixated on his quest to find the source of the melody. He followed the sound until it grew louder and more intense, until he finally came upon a clearing and saw a beautiful and mysterious woman standing in the center.\n",
      "\n",
      "The woman was unlike any Jack had ever seen before. She had long, flowing hair the color of the forest, and her eyes shone like the stars on a clear night. She was playing a beautiful and intricate instrument, and the melody that flowed from it was like nothing Jack had ever heard before.\n",
      "\n",
      "As Jack approached"
     ]
    }
   ],
   "source": [
    "content = \"\"\n",
    "for completion in local_llm.stream_complete(\"What is Dell Technologies?\"):\n",
    "    content += completion.delta\n",
    "    print(completion.delta, end=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# import gradio as gr\n",
    "\n",
    "# def stream_response_local(message, history):\n",
    "#     response = local_llm.stream_complete(message)\n",
    "#     res = \"\"\n",
    "#     for token in response:\n",
    "#         # print(token, end=\"\")\n",
    "#         res = str(res) + str(token.delta)\n",
    "#         yield res\n",
    "\n",
    "# with gr.Blocks() as demo1:\n",
    "#     gr.Markdown(\n",
    "#     \"\"\"\n",
    "#     <h1 style=\"text-align: center;\">Local HuggingFaceLLM Chatbot 💻📑✨</h3>\n",
    "#     \"\"\")\n",
    "#     with gr.Row(equal_height=False):\n",
    "#         with gr.Column():\n",
    "#             test = gr.ChatInterface(fn=stream_response_local)\n",
    "            \n",
    "\n",
    "# # demo.launch(server_name=\"0.0.0.0\", ssl_verify=False)\n",
    "# demo1.launch()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-06T08:16:28.211601Z",
     "iopub.status.busy": "2024-11-06T08:16:28.211208Z",
     "iopub.status.idle": "2024-11-06T08:16:41.857465Z",
     "shell.execute_reply": "2024-11-06T08:16:41.856286Z",
     "shell.execute_reply.started": "2024-11-06T08:16:28.211565Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/pty.py:89: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
      "  pid, fd = os.forkpty()\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install --upgrade --quiet llama-index-llms-nvidia llama-index-embeddings-nvidia llama-index-readers-file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-06T08:16:46.286446Z",
     "iopub.status.busy": "2024-11-06T08:16:46.285949Z",
     "iopub.status.idle": "2024-11-06T08:17:09.153707Z",
     "shell.execute_reply": "2024-11-06T08:17:09.152728Z",
     "shell.execute_reply.started": "2024-11-06T08:16:46.286394Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NVAPI Key (starts with nvapi-):  ······································································\n"
     ]
    }
   ],
   "source": [
    "import getpass\n",
    "import os\n",
    "\n",
    "# del os.environ['NVIDIA_API_KEY']  ## delete key and reset\n",
    "if os.environ.get(\"NVIDIA_API_KEY\", \"\").startswith(\"nvapi-\"):\n",
    "    print(\"Valid NVIDIA_API_KEY already in environment. Delete to reset\")\n",
    "else:\n",
    "    nvapi_key = getpass.getpass(\"NVAPI Key (starts with nvapi-): \")\n",
    "    assert nvapi_key.startswith(\n",
    "        \"nvapi-\"\n",
    "    ), f\"{nvapi_key[:5]}... is not a valid key\"\n",
    "    os.environ[\"NVIDIA_API_KEY\"] = nvapi_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-06T08:17:10.964169Z",
     "iopub.status.busy": "2024-11-06T08:17:10.963759Z",
     "iopub.status.idle": "2024-11-06T08:17:10.973799Z",
     "shell.execute_reply": "2024-11-06T08:17:10.973057Z",
     "shell.execute_reply.started": "2024-11-06T08:17:10.964134Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# llama-parse is async-first, running the async code in a notebook requires the use of nest_asyncio\n",
    "import nest_asyncio\n",
    "\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-06T08:17:57.755257Z",
     "iopub.status.busy": "2024-11-06T08:17:57.754536Z",
     "iopub.status.idle": "2024-11-06T08:18:02.426610Z",
     "shell.execute_reply": "2024-11-06T08:18:02.425734Z",
     "shell.execute_reply.started": "2024-11-06T08:17:57.755218Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once upon a time, in a small village nestled in the rolling hills of Provence, France, there lived a young girl named Sophie. Sophie was a curious and adventurous child, with a mop of curly brown hair and a smile that could light up the darkest of rooms.\n",
      "\n",
      "Sophie lived with her parents, Pierre and Colette, in a small stone cottage on the edge of the village. Her parents were kind and loving, and they encouraged Sophie's sense of wonder and curiosity. They told her stories of the ancient history of the region, of the Roman ruins that lay hidden beneath the earth, and of the mythical creatures that were said to roam the countryside.\n",
      "\n",
      "One day, while exploring the woods near her home, Sophie stumbled upon a hidden glade. In the center of the glade stood an enormous, ancient tree, its branches twisted and gnarled with age. Sophie felt drawn to the tree, as if it was calling to her. She reached out a hand to touch its trunk, and as she did, she felt a sudden jolt of energy.\n",
      "\n",
      "The next thing Sophie knew, she was standing in a different place. She was no longer in the woods, but in a bustling marketplace. The air was thick with the smells of fresh bread, roasting meats, and exotic spices. Sophie's eyes widened as she took in the sights and sounds of the market.\n",
      "\n",
      "A kind-faced old man approached her, his eyes twinkling with warmth. \"Welcome, young one,\" he said. \"I have been waiting for you. My name is Marcel, and I am a keeper of secrets. I sense that you have a special gift, one that will allow you to see the world in a way that others cannot.\"\n",
      "\n",
      "Sophie was skeptical, but Marcel's words sparked a sense of excitement within her. He took her on a journey through the market, pointing out hidden wonders and sharing stories of magic and mystery. As they walked, Sophie began to notice things she had never seen before - the way the light danced through the stalls, the colors of the fabrics and spices, and the way the people moved with a sense of purpose and joy.\n",
      "\n",
      "As the day wore on, Marcel led Sophie to a small, mysterious shop tucked away in a quiet alley. The sign above the door read \"Curios and Wonders,\" and the windows were filled with a dazzling array of strange and exotic objects. Marcel pushed open the door, and Sophie stepped inside.\n",
      "\n",
      "The shop was dimly lit, but Sophie's eyes adjusted quickly to the darkness. She saw shelves upon shelves of peculiar items - ancient artifacts, rare books, and strange devices that seemed to defy explanation. At the back of the shop, a figure sat hunched over a workbench, surrounded by a halo of candlelight.\n",
      "\n",
      "As Sophie approached, the figure looked up, revealing a face that was both familiar and strange. It was a woman with piercing green eyes and hair as white as snow. She smiled, and Sophie felt a sense of recognition, as if she had known this woman all her life.\n",
      "\n",
      "\"Welcome, Sophie,\" the woman said. \"I have been waiting for you. My name is Ariane, and I am a weaver of dreams. I sense that you have a special gift, one that will allow you to see the world in a way that others cannot. Are you ready to learn the secrets of the universe?\"\n",
      "\n",
      "Sophie nodded, and Ariane began to teach her the art of seeing the world in a new and magical way. As the days passed, Sophie's eyes were opened to wonders she had never imagined - the hidden patterns of the universe, the secrets of the human heart, and the magic that lay just beneath the surface of everyday life.\n",
      "\n",
      "And so, Sophie's journey began, a journey that would take her to the farthest reaches of the world and back again, a journey that would change her life forever."
     ]
    }
   ],
   "source": [
    "from llama_index.llms.nvidia import NVIDIA\n",
    "\n",
    "# connect to an chat NIM running at localhost:8080, spcecifying a specific model\n",
    "hosted_nim_llm = NVIDIA(\n",
    "    model=\"meta/llama-3.1-8b-instruct\"\n",
    ")\n",
    "\n",
    "content = \"\"\n",
    "for completion in hosted_nim_llm.stream_complete(\"What is Dell Technologies?\"):\n",
    "    content += completion.delta\n",
    "    print(completion.delta, end=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.llms.nvidia import NVIDIA\n",
    "\n",
    "# connect to an chat NIM running at localhost:8080, spcecifying a specific model\n",
    "local_nim_llm = NVIDIA(\n",
    "    base_url=\"http://localhost:8080/v1\", model=\"meta/llama-3.1-8b-instruct\"\n",
    ")\n",
    "\n",
    "content = \"\"\n",
    "for completion in local_nim_llm.stream_complete(\"Write me a story\"):\n",
    "    content += completion.delta\n",
    "    print(completion.delta, end=\"\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [],
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
