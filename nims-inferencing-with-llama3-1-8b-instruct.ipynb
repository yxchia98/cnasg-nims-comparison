{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NVIDIA Inference Microservices (NIMs) Comparison\n",
    "In this demo, utilizing the following code cells in this notebook, we will be interacting and comparing the various types of NIMs against traditional inferencing scenarios. \\\n",
    "**NOTE**: The cells in this workbook are mostly dependent on one another, please execute them serially in order."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install Dependencies\n",
    "We will be mainly interacting with HuggingFace, NVIDIA, and LlamaIndex. \\ \n",
    "Lets get started by installing the python dependencies needed for this demo. \\\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%pip install gradio\n",
    "%pip install llama-index-llms-huggingface\n",
    "%pip install llama-index-llms-huggingface-api\n",
    "%pip install --upgrade --quiet llama-index-llms-nvidia llama-index-embeddings-nvidia llama-index-readers-file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install \"transformers[torch]\" \"huggingface_hub[inference]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install llama-index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "import os\n",
    "os.environ[\"HF_TOKEN\"] = \"\"\n",
    "os.environ[\"NVIDIA_API_KEY\"] = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Environment\n",
    "Lets prepare and set up the environment we need for interacting with various APIs. \\\n",
    "We will need the following API Keys: \\\n",
    "**HuggingFace Access Token** - to retrieve and download the gated Llama-3.1-8B-Instruct model (appy for access [here](https://huggingface.co/meta-llama/Llama-3.1-8B-Instruct))\\\n",
    "**NGC Key** - to download NIMs containers to run On-Prem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "HF TOKEN (starts with hf_):  Â·Â·Â·Â·Â·Â·Â·Â·\n"
     ]
    }
   ],
   "source": [
    "hf_token = \"\"\n",
    "\n",
    "import getpass\n",
    "import os\n",
    "\n",
    "# del os.environ['HF_TOKEN']  ## delete key and reset\n",
    "if os.environ.get(\"HF_TOKEN\", \"\").startswith(\"hf_\"):\n",
    "    print(\"Valid HF_TOKEN already in environment. Delete to reset\")\n",
    "else:\n",
    "    hf_token = getpass.getpass(\"HF TOKEN (starts with hf_): \")\n",
    "    assert hf_token.startswith(\n",
    "        \"hf_\"\n",
    "    ), f\"{hf_token[:5]}... is not a valid key\"\n",
    "    os.environ[\"HF_TOKEN\"] = hf_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "NVAPI Key (starts with nvapi-):  Â·Â·Â·Â·Â·Â·Â·Â·\n"
     ]
    }
   ],
   "source": [
    "import getpass\n",
    "import os\n",
    "\n",
    "# del os.environ['NVIDIA_API_KEY']  ## delete key and reset\n",
    "if os.environ.get(\"NVIDIA_API_KEY\", \"\").startswith(\"nvapi-\"):\n",
    "    print(\"Valid NVIDIA_API_KEY already in environment. Delete to reset\")\n",
    "else:\n",
    "    nvapi_key = getpass.getpass(\"NVAPI Key (starts with nvapi-): \")\n",
    "    assert nvapi_key.startswith(\n",
    "        \"nvapi-\"\n",
    "    ), f\"{nvapi_key[:5]}... is not a valid key\"\n",
    "    os.environ[\"NVIDIA_API_KEY\"] = nvapi_key"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inferencing without NIMs\n",
    "In this section, running the following code cells will download models and serve them for inferncing without NIMs, directly using HuggingFace"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download Llama-3.1-8B-Instruct tokenizer & model from HuggingFace"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading the Llama-3.1-8B-Instruct Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    \"meta-llama/Llama-3.1-8B-Instruct\",\n",
    "    token=hf_token,\n",
    ")\n",
    "\n",
    "stopping_ids = [\n",
    "    tokenizer.eos_token_id,\n",
    "    tokenizer.convert_tokens_to_ids(\"<|eot_id|>\"),\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading the Llama-3.1-8B-Instruct Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.12/site-packages/pydantic/_internal/_fields.py:132: UserWarning: Field \"model_id\" in DeployedModel has conflict with protected namespace \"model_\".\n",
      "\n",
      "You may be able to resolve this warning by setting `model_config['protected_namespaces'] = ()`.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.12/site-packages/pydantic/_internal/_fields.py:132: UserWarning: Field \"model_name\" in HuggingFaceLLM has conflict with protected namespace \"model_\".\n",
      "\n",
      "You may be able to resolve this warning by setting `model_config['protected_namespaces'] = ()`.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.12/site-packages/pydantic/_internal/_fields.py:132: UserWarning: Field \"model_kwargs\" in HuggingFaceLLM has conflict with protected namespace \"model_\".\n",
      "\n",
      "You may be able to resolve this warning by setting `model_config['protected_namespaces'] = ()`.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.12/site-packages/pydantic/_internal/_fields.py:132: UserWarning: Field \"model_name\" in HuggingFaceInferenceAPI has conflict with protected namespace \"model_\".\n",
      "\n",
      "You may be able to resolve this warning by setting `model_config['protected_namespaces'] = ()`.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.12/site-packages/pydantic/_internal/_fields.py:132: UserWarning: Field \"model_name\" in TextGenerationInference has conflict with protected namespace \"model_\".\n",
      "\n",
      "You may be able to resolve this warning by setting `model_config['protected_namespaces'] = ()`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "71158e250eab44658dd6e29611826501",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# generate_kwargs parameters are taken from https://huggingface.co/meta-llama/Llama-3.1-8B-Instruct\n",
    "\n",
    "import torch\n",
    "from llama_index.llms.huggingface import HuggingFaceLLM\n",
    "\n",
    "# Optional quantization to 4bit\n",
    "# import torch\n",
    "# from transformers import BitsAndBytesConfig\n",
    "\n",
    "# quantization_config = BitsAndBytesConfig(\n",
    "#     load_in_4bit=True,\n",
    "#     bnb_4bit_compute_dtype=torch.float16,\n",
    "#     bnb_4bit_quant_type=\"nf4\",\n",
    "#     bnb_4bit_use_double_quant=True,\n",
    "# )\n",
    "\n",
    "local_llm = HuggingFaceLLM(\n",
    "    model_name=\"meta-llama/Llama-3.1-8B-Instruct\",\n",
    "    max_new_tokens=512,\n",
    "    model_kwargs={\n",
    "        \"token\": hf_token,\n",
    "        \"torch_dtype\": torch.bfloat16,  # comment this line and uncomment below to use 4bit\n",
    "        # \"quantization_config\": quantization_config\n",
    "    },\n",
    "    generate_kwargs={\n",
    "        \"do_sample\": True,\n",
    "        \"temperature\": 0.6,\n",
    "        \"top_p\": 0.9,\n",
    "    },\n",
    "    tokenizer_name=\"meta-llama/Llama-3.1-8B-Instruct\",\n",
    "    tokenizer_kwargs={\"token\": hf_token},\n",
    "    stopping_ids=stopping_ids,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample inferencing with locally-loaded HF model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Non-streaming Response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "assistant: Dell Technologies is an American multinational computer technology company that designs, develops, manufactures, markets, and services information technology (IT) products and services. The company was founded in 1984 by Michael Dell in his dorm room at the University of Texas at Austin.\n",
      "\n",
      "Dell is one of the largest technology companies in the world and is known for its wide range of products and services, including:\n",
      "\n",
      "1. **Personal Computers**: Dell offers a variety of desktops, laptops, and tablets for both personal and business use.\n",
      "2. **Servers**: Dell provides a range of servers for data centers, including rack servers, blade servers, and storage systems.\n",
      "3. **Storage**: Dell offers a variety of storage solutions, including hard disk drives, solid-state drives, and storage arrays.\n",
      "4. **Networking**: Dell provides a range of networking products, including switches, routers, and network security solutions.\n",
      "5. **Software**: Dell offers a range of software solutions, including operating systems, security software, and data management tools.\n",
      "6. **Services**: Dell provides a range of services, including consulting, deployment, and support services for its products.\n",
      "\n",
      "Dell has a strong presence in the IT industry and is known for its innovative products, competitive pricing, and customer-centric approach. The company has a global presence with operations in over 180 countries and a diverse customer base that includes individuals, businesses, governments, and educational institutions.\n",
      "\n",
      "In 2016, Dell went private in a $67 billion deal led by Michael Dell and Silver Lake Partners. In 2018, Dell acquired EMC Corporation, a leading provider of data storage and management solutions, in a $67 billion deal. This acquisition expanded Dell's portfolio of IT products and services and solidified its position as a leading player in the IT industry.\n",
      "\n",
      "Today, Dell Technologies is a global leader in the IT industry, with a diverse range of products and services that cater to the needs of individuals, businesses, and organizations around the world.\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core.llms import ChatMessage\n",
    "\n",
    "# response = local_llm.complete(\"What is Dell Technologies?\")\n",
    "# print(response)\n",
    "\n",
    "messages = [\n",
    "    ChatMessage(role=\"system\", content=\"You are a helpful assistant.\"),\n",
    "    ChatMessage(role=\"user\", content=\"What is Dell Technologies?\"),\n",
    "]\n",
    "response = local_llm.chat(messages)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Streaming Response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dell Technologies is a multinational computer technology company that designs, develops, manufactures, markets, and services computers and other electronic devices. The company was founded in 1984 by Michael Dell in his dorm room at the University of Texas at Austin.\n",
      "\n",
      "Dell is one of the largest technology companies in the world, and it has a diverse portfolio of products and services that cater to various industries, including:\n",
      "\n",
      "1. **Personal Computers**: Dell offers a range of consumer and business PCs, including desktops, laptops, and tablets.\n",
      "2. **Servers and Storage**: Dell provides a wide range of servers, storage systems, and networking solutions for businesses and data centers.\n",
      "3. **Networking and Security**: Dell offers a variety of networking and security solutions, including switches, routers, and firewalls.\n",
      "4. **Data Analytics and AI**: Dell provides data analytics and artificial intelligence solutions, including data storage, processing, and visualization tools.\n",
      "5. **Cloud Computing**: Dell offers a range of cloud computing services, including public, private, and hybrid cloud solutions.\n",
      "6. **Virtualization**: Dell provides virtualization software and solutions, including VMware and Citrix.\n",
      "7. **Services**: Dell offers a range of services, including consulting, deployment, and support services for its products.\n",
      "\n",
      "Dell has a strong presence in various markets, including:\n",
      "\n",
      "1. **Enterprise**: Dell serves large businesses and enterprises, providing them with customized solutions for their IT needs.\n",
      "2. **Small and Medium-sized Businesses (SMBs)**: Dell offers a range of products and services for SMBs, including PCs, servers, and storage solutions.\n",
      "3. **Consumer**: Dell sells a range of consumer products, including PCs, laptops, and tablets.\n",
      "\n",
      "In 2016, Dell went private in a $67 billion deal led by its founder Michael Dell and private equity firm Silver Lake Partners. In 2019, Dell acquired EMC Corporation, a leading data storage and management company, for $67 billion. Today, Dell Technologies is a global leader in the technology industry, with a presence in over 180 countries and a diverse portfolio of products and services."
     ]
    }
   ],
   "source": [
    "from llama_index.core.llms import ChatMessage\n",
    "\n",
    "# content = \"\"\n",
    "# for completion in local_llm.stream_complete(\"What is Dell Technologies?\"):\n",
    "#     content += completion.delta\n",
    "#     print(completion.delta, end=\"\")\n",
    "\n",
    "messages = [\n",
    "    ChatMessage(role=\"system\", content=\"You are a helpful assistant.\"),\n",
    "    ChatMessage(role=\"user\", content=\"What is Dell Technologies?\"),\n",
    "]\n",
    "\n",
    "content = \"\"\n",
    "for completion in local_llm.stream_chat(messages):\n",
    "    content += completion.delta\n",
    "    print(completion.delta, end=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a Gradio UI for interacting with Local HuggingFace LLM\n",
    "We will now create a Gradio frontend to consume the locally-loaded HF model. \\\n",
    "Launching Gradio clients will consume ports incrementally starting from port `7860` \\\n",
    "As this is the first demo we are launching concurrently, it will be served on port `7860`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.12/site-packages/gradio/components/chatbot.py:229: UserWarning: The 'tuples' format for chatbot messages is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style 'role' and 'content' keys.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://0.0.0.0:7860\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gradio as gr\n",
    "from IPython.display import Markdown\n",
    "from llama_index.core.llms import ChatMessage\n",
    "\n",
    "css = \"\"\"\n",
    ".app-interface {\n",
    "    height:80vh;\n",
    "}\n",
    ".chat-interface {\n",
    "    height: 75vh;\n",
    "}\n",
    ".file-interface {\n",
    "    height: 40vh;\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "def stream_response_local(message, history):\n",
    "    messages = [\n",
    "        ChatMessage(role=\"system\", content=\"You are a helpful assistant.\"),\n",
    "        ChatMessage(role=\"user\", content=message),\n",
    "    ]\n",
    "    response = local_llm.stream_chat(messages)\n",
    "    res = \"\"\n",
    "    for token in response:\n",
    "        # print(token, end=\"\")\n",
    "        res = str(res) + str(token.delta)\n",
    "        yield res\n",
    "\n",
    "with gr.Blocks(css=css) as demo1:\n",
    "    gr.Markdown(\n",
    "    \"\"\"\n",
    "    <h1 style=\"text-align: center;\">Local HuggingFaceLLM Chatbot ðŸ’»ðŸ“‘âœ¨</h3>\n",
    "    \"\"\")\n",
    "    with gr.Row(equal_height=False, elem_classes=[\"app-interface\"]):\n",
    "        with gr.Column(scale=4, elem_classes=[\"chat-interface\"]):\n",
    "            test = gr.ChatInterface(fn=stream_response_local)\n",
    "            \n",
    "\n",
    "# Markdown(str('''\n",
    "# \\\n",
    "# **NOTE: please dont use the above URL, use this instead: [{gradio_url}]({gradio_url})**\n",
    "# '''.format(gradio_url='/proxy/7860')))\n",
    "\n",
    "demo1.launch(server_name=\"0.0.0.0\", ssl_verify=False, inline=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE: To access the On-Prem HuggingFace LLM Demo, please dont use the above URL, use this instead: `http://<YOUR-VM-IP-ADDRESS>:7860`**\\\n",
    "The gradio frontend for this specific demo is served on port `7860` \\\n",
    "For example, if my VM IP address is 172.27.193.230, go to  `http://172.27.193.230:7860`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inferencing with On-Prem NVIDIA Inference Microservices (NIM)\n",
    "In this section, running the following code cells will consume a locally-deployed Llama-3.1-8B-Instruct NIM container and serve them for inferncing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# llama-parse is async-first, running the async code in a notebook requires the use of nest_asyncio\n",
    "import nest_asyncio\n",
    "\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample inferencing with On-Prem NIM model\n",
    "In the following code cell, we will first specify the NIM endpoint serving Llama-3.1-8B-Instruct model, and execute a sample inference afterwards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dell Technologies is a multinational computer technology company that designs, develops, manufactures, markets, and supports computers and other electronic devices. The company was founded in 1984 by Michael Dell and is headquartered in Round Rock, Texas, USA.\n",
      "\n",
      "Dell Technologies is a leading provider of a wide range of products and services, including:\n",
      "\n",
      "1. **Personal computers**: Desktops, laptops, tablets, and mobile devices.\n",
      "2. **Servers**: Data center servers, storage systems, and networking equipment.\n",
      "3. **Storage**: Storage solutions, including hard disk drives, solid-state drives, and storage arrays.\n",
      "4. **Networking**: Networking equipment, including switches, routers, and wireless access points.\n",
      "5. **Virtualization**: Virtualization software and services, including VMware.\n",
      "6. **Cloud computing**: Cloud infrastructure, platform, and software as a service (IaaS, PaaS, SaaS).\n",
      "7. **Cybersecurity**: Cybersecurity solutions, including threat detection, incident response, and security consulting.\n",
      "8. **Artificial intelligence**: AI and machine learning solutions, including data analytics and predictive maintenance.\n",
      "\n",
      "Dell Technologies has a strong presence in various industries, including:\n",
      "\n",
      "1. **Enterprise**: Large businesses and organizations.\n",
      "2. **Small and medium-sized businesses (SMBs)**: Smaller businesses and organizations.\n",
      "3. **Government**: Government agencies and institutions.\n",
      "4. **Education**: Educational institutions and organizations.\n",
      "\n",
      "The company has a significant global presence, with operations in over 180 countries and a workforce of over 130,000 employees.\n",
      "\n",
      "In 2016, Dell Technologies acquired EMC Corporation, a leading provider of data storage and management solutions, in a $67 billion deal. This acquisition expanded Dell's offerings in the data center and cloud computing spaces.\n",
      "\n",
      "Today, Dell Technologies is a leading player in the technology industry, known for its innovative products, services, and solutions that help organizations of all sizes to transform their businesses and improve their operations."
     ]
    }
   ],
   "source": [
    "from llama_index.llms.nvidia import NVIDIA\n",
    "from llama_index.core.llms import ChatMessage\n",
    "\n",
    "# connect to an chat NIM running at localhost:8000, spcecifying a specific model\n",
    "local_nim_llm = NVIDIA(\n",
    "    base_url=\"http://localhost:8000/v1\", model=\"meta/llama-3.1-8b-instruct\"\n",
    ")\n",
    "\n",
    "# content = \"\"\n",
    "# for completion in local_nim_llm.stream_complete(\"What is Dell Technologies?\"):\n",
    "#     content += completion.delta\n",
    "#     print(completion.delta, end=\"\")\n",
    "\n",
    "messages = [\n",
    "    ChatMessage(role=\"system\", content=\"You are a helpful assistant.\"),\n",
    "    ChatMessage(role=\"user\", content=\"What is Dell Technologies?\"),\n",
    "]\n",
    "\n",
    "content = \"\"\n",
    "for completion in local_nim_llm.stream_chat(messages):\n",
    "    content += completion.delta\n",
    "    print(completion.delta, end=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a Gradio UI for interacting with On-Prem NIM\n",
    "We will now create a Gradio frontend to consume the deployed On-Prem NIM model. \\\n",
    "Launching Gradio clients will consume ports incrementally starting from port `7860` \\\n",
    "As this is the second demo we are launching concurrently, it will be served on port `7861`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.12/site-packages/gradio/components/chatbot.py:229: UserWarning: The 'tuples' format for chatbot messages is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style 'role' and 'content' keys.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://0.0.0.0:7861\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gradio as gr\n",
    "from IPython.display import Markdown\n",
    "from llama_index.core.llms import ChatMessage\n",
    "\n",
    "css = \"\"\"\n",
    ".app-interface {\n",
    "    height:80vh;\n",
    "}\n",
    ".chat-interface {\n",
    "    height: 75vh;\n",
    "}\n",
    ".file-interface {\n",
    "    height: 40vh;\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "def stream_response_local_nim(message, history):\n",
    "    messages = [\n",
    "        ChatMessage(role=\"system\", content=\"You are a helpful assistant.\"),\n",
    "        ChatMessage(role=\"user\", content=message),\n",
    "    ]\n",
    "    response = local_nim_llm.stream_chat(messages)\n",
    "    res = \"\"\n",
    "    for token in response:\n",
    "        # print(token, end=\"\")\n",
    "        res = str(res) + str(token.delta)\n",
    "        yield res\n",
    "\n",
    "with gr.Blocks(css=css) as demo3:\n",
    "    gr.Markdown(\n",
    "    \"\"\"\n",
    "    <h1 style=\"text-align: center;\">Local NIM Chatbot ðŸ’»ðŸ“‘âœ¨</h3>\n",
    "    \"\"\")\n",
    "    with gr.Row(equal_height=False, elem_classes=[\"app-interface\"]):\n",
    "        with gr.Column(scale=4, elem_classes=[\"chat-interface\"]):\n",
    "            test = gr.ChatInterface(fn=stream_response_local_nim)\n",
    "\n",
    "\n",
    "\n",
    "Markdown(str('''\n",
    "\\\n",
    "**NOTE: please dont use the above URL, use this instead: [{gradio_url}]({gradio_url})**\n",
    "'''.format(gradio_url='/proxy/7862')))\n",
    "\n",
    "demo3.launch(server_name=\"0.0.0.0\", ssl_verify=False, inline=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE: To access the On-Prem NIMs Demo, please dont use the above URL, use this instead: `http://<YOUR-VM-IP-ADDRESS>:7861`**\\\n",
    "The gradio frontend for this specific demo is served on port `7861` \\\n",
    "For example, if my VM IP address is 172.27.193.230, go to  `http://172.27.193.230:7861`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inferencing with Hosted NVIDIA Inference Microservices (NIM)\n",
    "In this section, running the following code cells will consume a cloud-hosted Llama-3.1-8B-Instruct NIM."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample inferencing with Hosted NIM model\n",
    "In the following code cell, we will first specify the NIM endpoint serving Llama-3.1-8B-Instruct model, and execute a sample inference afterwards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dell Technologies is a multinational computer technology company that designs, manufactures, sells, and supports computers and other electronic devices. The company was founded in 1984 by Michael Dell and is headquartered in Round Rock, Texas.\n",
      "\n",
      "Dell Technologies is a leading provider of a wide range of products and services, including:\n",
      "\n",
      "1. **Personal computers**: Desktops, laptops, tablets, and 2-in-1 devices for consumers and businesses.\n",
      "2. **Servers**: Data center servers, storage systems, and networking equipment for businesses and organizations.\n",
      "3. **Storage**: External hard drives, solid-state drives, and storage arrays for data storage and management.\n",
      "4. **Networking**: Switches, routers, and other networking equipment for businesses and organizations.\n",
      "5. **Virtualization**: Software and services for virtualizing and managing IT infrastructure.\n",
      "6. **Cloud computing**: Cloud-based services, including infrastructure as a service (IaaS), platform as a service (PaaS), and software as a service (SaaS).\n",
      "7. **Security**: Cybersecurity solutions, including threat detection, incident response, and security consulting services.\n",
      "8. **Artificial intelligence**: AI-powered solutions for businesses, including AI-driven analytics, automation, and decision-making.\n",
      "\n",
      "Dell Technologies also offers a range of services, including:\n",
      "\n",
      "1. **Managed services**: Outsourced IT management and support for businesses.\n",
      "2. **Professional services**: Consulting, implementation, and integration services for IT projects.\n",
      "3. **Support services**: Technical support and maintenance services for Dell products.\n",
      "\n",
      "In 2016, Dell acquired EMC Corporation, a leading provider of data storage and management solutions, in a deal worth $67 billion. The acquisition expanded Dell's offerings in the data center and cloud computing spaces.\n",
      "\n",
      "Today, Dell Technologies is a global company with operations in over 180 countries and a workforce of over 130,000 employees. It is one of the largest and most successful technology companies in the world."
     ]
    }
   ],
   "source": [
    "from llama_index.llms.nvidia import NVIDIA\n",
    "from llama_index.core.llms import ChatMessage\n",
    "\n",
    "# connect to an chat NIM running at localhost:8080, spcecifying a specific model\n",
    "hosted_nim_llm = NVIDIA(\n",
    "    model=\"meta/llama-3.1-8b-instruct\"\n",
    ")\n",
    "\n",
    "# content = \"\"\n",
    "# for completion in hosted_nim_llm.stream_complete(\"What is Dell Technologies?\"):\n",
    "#     content += completion.delta\n",
    "#     print(completion.delta, end=\"\")\n",
    "\n",
    "messages = [\n",
    "    ChatMessage(role=\"system\", content=\"You are a helpful assistant.\"),\n",
    "    ChatMessage(role=\"user\", content=\"What is Dell Technologies?\"),\n",
    "]\n",
    "\n",
    "content = \"\"\n",
    "for completion in hosted_nim_llm.stream_chat(messages):\n",
    "    content += completion.delta\n",
    "    print(completion.delta, end=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a Gradio UI for interacting with Hosted NIM\n",
    "We will now create a Gradio frontend to consume the deployed hosted NIM model. \\\n",
    "Launching Gradio clients will consume ports incrementally starting from port `7860` \\\n",
    "As this is the second demo we are launching concurrently, it will be served on port `7862`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.12/site-packages/gradio/components/chatbot.py:229: UserWarning: The 'tuples' format for chatbot messages is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style 'role' and 'content' keys.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://0.0.0.0:7862\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "import gradio as gr\n",
    "from IPython.display import Markdown\n",
    "from llama_index.core.llms import ChatMessage\n",
    "\n",
    "css = \"\"\"\n",
    ".app-interface {\n",
    "    height:80vh;\n",
    "}\n",
    ".chat-interface {\n",
    "    height: 75vh;\n",
    "}\n",
    ".file-interface {\n",
    "    height: 40vh;\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "Markdown(str('''\n",
    "\\\n",
    "**NOTE: please dont use the above URL, use this instead: [{gradio_url}]({gradio_url})**\n",
    "'''.format(gradio_url='/proxy/7861')))\n",
    "\n",
    "def stream_response_hosted_nim(message, history):\n",
    "    messages = [\n",
    "        ChatMessage(role=\"system\", content=\"You are a helpful assistant.\"),\n",
    "        ChatMessage(role=\"user\", content=message),\n",
    "    ]\n",
    "    response = hosted_nim_llm.stream_chat(messages)\n",
    "    res = \"\"\n",
    "    for token in response:\n",
    "        # print(token, end=\"\")\n",
    "        res = str(res) + str(token.delta)\n",
    "        yield res\n",
    "\n",
    "with gr.Blocks(css=css) as demo2:\n",
    "    gr.Markdown(\n",
    "    \"\"\"\n",
    "    <h1 style=\"text-align: center;\">Hosted NIM Chatbot ðŸ’»ðŸ“‘âœ¨</h3>\n",
    "    \"\"\")\n",
    "    with gr.Row(equal_height=False, elem_classes=[\"app-interface\"]):\n",
    "        with gr.Column(scale=4, elem_classes=[\"chat-interface\"]):\n",
    "            test = gr.ChatInterface(fn=stream_response_hosted_nim)\n",
    "            \n",
    "\n",
    "demo2.launch(server_name=\"0.0.0.0\", ssl_verify=False, inline=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE: To access the Hosted NIMs Demo, please dont use the above URL, use this instead: `http://<YOUR-VM-IP-ADDRESS>:7862`**\\\n",
    "The gradio frontend for this specific demo is served on port `7862` \\\n",
    "For example, if my VM IP address is 172.27.193.230, go to  `http://172.27.193.230:7862`"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [],
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
