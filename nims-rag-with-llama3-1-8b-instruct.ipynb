{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e13e2b76-7b6f-451c-bbc0-ef777279dee8",
   "metadata": {},
   "source": [
    "## Install Dependencies\n",
    "We will be mainly interacting with HuggingFace, NVIDIA, and LlamaIndex. \\ \n",
    "Lets get started by installing the python dependencies needed for this demo. \\\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "588a45a5-c07c-4df3-9470-9ad05be77067",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%pip install gradio\n",
    "%pip install llama-index-llms-huggingface\n",
    "%pip install llama-index-llms-huggingface-api\n",
    "%pip install llama-index-embeddings-huggingface\n",
    "%pip install llama-index-embeddings-huggingface-api\n",
    "%pip install --upgrade --quiet llama-index-llms-nvidia llama-index-embeddings-nvidia llama-index-readers-file llama-index llama-index-readers-web"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be627c0e-aba3-4869-9421-73b2b027c942",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install \"transformers[torch]\" \"huggingface_hub[inference]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8085cd3f-6eb8-48d0-8ac8-b820e2ae98a7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install llama-index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c39609f-1020-4c30-96af-6fabd174941f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "import os\n",
    "os.environ[\"HF_TOKEN\"] = \"\"\n",
    "os.environ[\"NVIDIA_API_KEY\"] = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ede58d19-2346-406d-b9b0-3f3f49eed854",
   "metadata": {},
   "source": [
    "## Prepare Environment\n",
    "Lets prepare and set up the environment we need for interacting with various APIs. \\\n",
    "We will need the following API Keys: \\\n",
    "**HuggingFace Access Token** - to retrieve and download the gated Llama-3.1-8B-Instruct model (appy for access [here](https://huggingface.co/meta-llama/Llama-3.1-8B-Instruct))\\\n",
    "**NGC Key** - to download NIMs containers to run On-Prem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "60db213d-9b45-4d82-8313-9ea94dafaee5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "HF TOKEN (starts with hf_):  Â·Â·Â·Â·Â·Â·Â·Â·\n"
     ]
    }
   ],
   "source": [
    "hf_token = \"\"\n",
    "\n",
    "import getpass\n",
    "import os\n",
    "\n",
    "# del os.environ['HF_TOKEN']  ## delete key and reset\n",
    "if os.environ.get(\"HF_TOKEN\", \"\").startswith(\"hf_\"):\n",
    "    print(\"Valid HF_TOKEN already in environment. Delete to reset\")\n",
    "else:\n",
    "    hf_token = getpass.getpass(\"HF TOKEN (starts with hf_): \")\n",
    "    assert hf_token.startswith(\n",
    "        \"hf_\"\n",
    "    ), f\"{hf_token[:5]}... is not a valid key\"\n",
    "    os.environ[\"HF_TOKEN\"] = hf_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8c811987-388e-4c73-902c-e2bede9cfa9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "NVAPI Key (starts with nvapi-):  Â·Â·Â·Â·Â·Â·Â·Â·\n"
     ]
    }
   ],
   "source": [
    "import getpass\n",
    "import os\n",
    "\n",
    "# del os.environ['NVIDIA_API_KEY']  ## delete key and reset\n",
    "if os.environ.get(\"NVIDIA_API_KEY\", \"\").startswith(\"nvapi-\"):\n",
    "    print(\"Valid NVIDIA_API_KEY already in environment. Delete to reset\")\n",
    "else:\n",
    "    nvapi_key = getpass.getpass(\"NVAPI Key (starts with nvapi-): \")\n",
    "    assert nvapi_key.startswith(\n",
    "        \"nvapi-\"\n",
    "    ), f\"{nvapi_key[:5]}... is not a valid key\"\n",
    "    os.environ[\"NVIDIA_API_KEY\"] = nvapi_key"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2fbc377-fd2d-469a-a672-76c1d06aa22b",
   "metadata": {},
   "source": [
    "## Inferencing with On-Prem NVIDIA Inference Microservices (NIM)\n",
    "In this section, running the following code cells will consume a locally-deployed Llama-3.1-8B-Instruct NIM container and serve them for inferncing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d89bbb15-8e2d-4572-b392-5e8ebe7449d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# llama-parse is async-first, running the async code in a notebook requires the use of nest_asyncio\n",
    "import nest_asyncio\n",
    "\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dec04231-5ab0-4ac2-bf35-d767580a3fbd",
   "metadata": {},
   "source": [
    "### Sample inferencing with On-Prem NIM model\n",
    "In the following code cell, we will first specify the NIM endpoint serving Llama-3.1-8B-Instruct model, and execute a sample inference afterwards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "49eb25eb-7fb9-40d2-8fdb-82fd02b4c685",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dell Technologies is a multinational computer technology company that designs, develops, manufactures, markets, and supports computers and other electronic devices. The company was founded in 1984 by Michael Dell and is headquartered in Round Rock, Texas, USA.\n",
      "\n",
      "Dell Technologies is a leading provider of a wide range of products and services, including:\n",
      "\n",
      "1. **Personal computers**: Desktops, laptops, tablets, and mobile devices.\n",
      "2. **Servers**: Data center servers, storage systems, and networking equipment.\n",
      "3. **Storage**: Storage solutions, including hard disk drives, solid-state drives, and storage arrays.\n",
      "4. **Networking**: Networking equipment, including switches, routers, and wireless access points.\n",
      "5. **Virtualization**: Virtualization software and services, including VMware.\n",
      "6. **Cloud computing**: Cloud infrastructure, platform, and software as a service (IaaS, PaaS, SaaS).\n",
      "7. **Cybersecurity**: Cybersecurity solutions, including threat detection, incident response, and security consulting.\n",
      "8. **Artificial intelligence**: AI and machine learning solutions, including data analytics and predictive maintenance.\n",
      "\n",
      "Dell Technologies is a global company with operations in over 180 countries and a workforce of over 130,000 employees. The company has a strong presence in the enterprise market, serving large corporations, governments, and small and medium-sized businesses.\n",
      "\n",
      "In 2016, Dell acquired EMC Corporation, a leading provider of data storage and management solutions, in a deal worth $67 billion. This acquisition expanded Dell's portfolio of products and services, making it one of the largest and most diversified technology companies in the world.\n",
      "\n",
      "Today, Dell Technologies is a leader in the technology industry, known for its innovative products, services, and solutions that help organizations of all sizes to transform their businesses and improve their operations."
     ]
    }
   ],
   "source": [
    "from llama_index.llms.nvidia import NVIDIA\n",
    "from llama_index.core.llms import ChatMessage\n",
    "\n",
    "# connect to an chat NIM running at localhost:8000, spcecifying a specific model\n",
    "local_nim_llm = NVIDIA(\n",
    "    base_url=\"http://localhost:8000/v1\", model=\"meta/llama-3.1-8b-instruct\"\n",
    ")\n",
    "\n",
    "# content = \"\"\n",
    "# for completion in local_nim_llm.stream_complete(\"What is Dell Technologies?\"):\n",
    "#     content += completion.delta\n",
    "#     print(completion.delta, end=\"\")\n",
    "\n",
    "messages = [\n",
    "    ChatMessage(role=\"system\", content=\"You are a helpful assistant.\"),\n",
    "    ChatMessage(role=\"user\", content=\"What is Dell Technologies?\"),\n",
    "]\n",
    "\n",
    "content = \"\"\n",
    "for completion in local_nim_llm.stream_chat(messages):\n",
    "    content += completion.delta\n",
    "    print(completion.delta, end=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26c7acf1-6806-4b39-9c9e-30573b12359d",
   "metadata": {},
   "source": [
    "## Retrieval Augmented Generation (RAG) with On-Prem NIM\n",
    "In this section, we will code up RAG utilizing on-prem NIM, and eventually present those in a interactive chatbot where users can:\\\n",
    "1. scrape websites for additional content\n",
    "2. upload files for additional content\n",
    "3. Choose between utilizing knowledge base or not"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f94d6c45-4d49-4736-b287-98dc58f8f201",
   "metadata": {},
   "source": [
    "### Writing the logic for RAG\n",
    "In the following code dell, we will develop a custom LlamaIndex Query Engine Class which does the following:\n",
    "- Sets the deployed NIM endpoint for LLM inferencing\n",
    "- Loads and sets a local embedding model from HuggingFace \n",
    "- Instantiates a in-memory Vector Database\n",
    "- Instantiates and connects the LLM, Embedding Model, and Vector Databases together into a Query Engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4fff8467-21bb-4104-a651-8f2b7739875a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader, Settings\n",
    "from llama_index.readers.web import SimpleWebPageReader\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "from llama_index.llms.nvidia import NVIDIA\n",
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "os.environ['GRADIO_TEMP_DIR'] = \"/home/jovyan/work/gradio\"\n",
    "\n",
    "RAG_UPLOAD_FOLDER = \"/home/jovyan/work/rag-documents/\"\n",
    "\n",
    "\n",
    "class Custom_Query_Engine():\n",
    "    def __init__(self):\n",
    "        self.SYSTEM_PROMPT = \"\"\"You are an AI assistant that answers questions in a friendly manner. Here are some rules you always follow:\n",
    "        - Generate human readable output, avoid creating output with gibberish text.\n",
    "        - Make use of the additional context given to provide better answers.\n",
    "        - Elaborate on your responses based on the context given.\n",
    "        - Give as much detail as you can to help the user with the query.\n",
    "        \"\"\"\n",
    "        \n",
    "        self.llm = NVIDIA(\n",
    "            base_url=\"http://localhost:8000/v1\", model=\"meta/llama-3.1-8b-instruct\"\n",
    "        )\n",
    "\n",
    "        # self.embed_model = IpexLLMEmbedding(model_name=\"/llm-models/hf-models/bge-small-en-v1.5\", trust_remote_code=True)\n",
    "        self.embed_model = HuggingFaceEmbedding(model_name=\"BAAI/bge-small-en-v1.5\")\n",
    "            \n",
    "        Settings.llm = self.llm\n",
    "        Settings.embed_model = self.embed_model\n",
    "        \n",
    "        Path(RAG_UPLOAD_FOLDER).mkdir(parents=True, exist_ok=True)\n",
    "        self.use_rag = False\n",
    "\n",
    "    def toggle_rag(self, toggle):\n",
    "        self.use_rag = toggle\n",
    "        return self.use_rag\n",
    "\n",
    "    def get_rag_toggle(self):\n",
    "        return self.use_rag\n",
    "\n",
    "    def query(self, message):\n",
    "        return self.query_engine.query(message)\n",
    "\n",
    "    def query_without_rag(self, message):\n",
    "        messages = [\n",
    "            ChatMessage(role=\"system\", content=\"You are a helpful assistant.\"),\n",
    "            ChatMessage(role=\"user\", content=message),\n",
    "        ]\n",
    "        return self.llm.stream_chat(messages)\n",
    "\n",
    "    def reload_scraped(self, documents):\n",
    "\n",
    "        try:\n",
    "            del self.query_engine\n",
    "        except:\n",
    "            print(\"instantiating new query engine\")\n",
    "        else:\n",
    "            print(\"re-creating query engine\")\n",
    "\n",
    "        try:\n",
    "            del self.index\n",
    "        except:\n",
    "            print(\"instantiating new index\")\n",
    "        else:\n",
    "            print(\"re-creating index\")\n",
    "            \n",
    "        self.index = VectorStoreIndex.from_documents(documents, show_progress=True)\n",
    "        self.query_engine = self.index.as_query_engine(streaming=True, similarity_top_k=6)\n",
    "\n",
    "    def reload_uploaded(self, path):\n",
    "\n",
    "        try:\n",
    "            del self.query_engine\n",
    "        except:\n",
    "            print(\"instantiating new query engine\")\n",
    "        else:\n",
    "            print(\"re-creating query engine\")\n",
    "\n",
    "        try:\n",
    "            del self.index\n",
    "        except:\n",
    "            print(\"instantiating new index\")\n",
    "        else:\n",
    "            print(\"re-creating index\")\n",
    "            \n",
    "        self.documents = SimpleDirectoryReader(RAG_UPLOAD_FOLDER).load_data()\n",
    "        self.index = VectorStoreIndex.from_documents(self.documents, show_progress=True)\n",
    "        self.query_engine = self.index.as_query_engine(streaming=True, similarity_top_k=6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b557b8d3-06a1-45fd-ab29-e9071d3a4ccb",
   "metadata": {},
   "source": [
    "### Writing additional helper functions for Gradio RAG Chatbot\n",
    "Next, we will write some additional helper functions to take in information passed via our Gradio Frontend, process these information and feed it into our query engine. These functions include:\n",
    "- Toggling of knowledge base\n",
    "- Scraping and vectorizing files grabbed from custom URLs\n",
    "- Vectorizig files uploaded by the user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e3f855e8-6b5f-432e-ae31-04d7b7dab2b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "import shutil\n",
    "import glob\n",
    "import os\n",
    "\n",
    "def stream_response(message, history):\n",
    "    print(f\"current RAG toggle is {query_engine.get_rag_toggle()}\")\n",
    "    if query_engine.get_rag_toggle():\n",
    "        print('using RAG')\n",
    "        response = query_engine.query(message)\n",
    "        print(response.source_nodes[0].get_content())\n",
    "        res = \"\"\n",
    "        for token in response.response_gen:\n",
    "            # print(token, end=\"\")\n",
    "            res = str(res) + str(token)\n",
    "            yield res\n",
    "    else:\n",
    "        print('not using RAG')\n",
    "        response = query_engine.query_without_rag(message)\n",
    "        res = \"\"\n",
    "        for token in response:\n",
    "            # print(token, end=\"\")\n",
    "            res = str(res) + str(token.delta)\n",
    "            yield res\n",
    "\n",
    "def vectorize_scrape(url, progress=gr.Progress()):\n",
    "    Path(RAG_UPLOAD_FOLDER).mkdir(parents=True, exist_ok=True)\n",
    "    UPLOAD_FOLDER = RAG_UPLOAD_FOLDER\n",
    "\n",
    "    prev_files = glob.glob(f\"{UPLOAD_FOLDER}*\")\n",
    "    for f in prev_files:\n",
    "        os.remove(f)\n",
    "\n",
    "    if not url:\n",
    "        return []\n",
    "    \n",
    "    documents = SimpleWebPageReader(html_to_text=True).load_data([url])\n",
    "\n",
    "\n",
    "    query_engine.reload_scraped(documents)\n",
    "    \n",
    "    return url\n",
    "\n",
    "def vectorize_uploads(files, progress=gr.Progress()):\n",
    "    Path(RAG_UPLOAD_FOLDER).mkdir(parents=True, exist_ok=True)\n",
    "    UPLOAD_FOLDER = RAG_UPLOAD_FOLDER\n",
    "\n",
    "    prev_files = glob.glob(f\"{UPLOAD_FOLDER}*\")\n",
    "    for f in prev_files:\n",
    "        os.remove(f)\n",
    "\n",
    "    if not files:\n",
    "        return []\n",
    "    \n",
    "    file_paths = [file.name for file in files]\n",
    "\n",
    "    for file in files:\n",
    "        shutil.copy(file.name, UPLOAD_FOLDER)\n",
    "\n",
    "    query_engine.reload_uploaded(UPLOAD_FOLDER)\n",
    "    \n",
    "    return file_paths\n",
    "\n",
    "def toggle_knowledge_base(use_rag):\n",
    "    print(f\"toggling use knowledge base to {use_rag}\")\n",
    "    query_engine.toggle_rag(use_rag)\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55a9908c-141b-40d0-bd11-eedef6c74a62",
   "metadata": {},
   "source": [
    "### Creating a Gradio UI for interacting with On-Prem NIM\n",
    "We will now create a Gradio frontend to consume the deployed On-Prem NIM model. \\\n",
    "Launching Gradio clients will consume ports incrementally starting from port `7860` \\\n",
    "As this is the second demo we are launching concurrently, it will be served on port `7860`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1f90e04a-ed23-4387-aea8-12ea2a564dd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.12/site-packages/gradio/components/chatbot.py:229: UserWarning: The 'tuples' format for chatbot messages is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style 'role' and 'content' keys.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://0.0.0.0:7860\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current RAG toggle is False\n",
      "not using RAG\n",
      "instantiating new query engine\n",
      "instantiating new index\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "30b38688d87e4699bc3abddc1ff9781d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Parsing nodes:   0%|          | 0/22 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d0df50224e614663a3f295cddb04a370",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating embeddings:   0%|          | 0/22 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "toggling use knowledge base to True\n",
      "current RAG toggle is True\n",
      "using RAG\n",
      "Validation findings  \n",
      " \n",
      "19 Generative AI in the Enterprise â€“ Model Training  \n",
      "A Scalable and Modular Production Infrastructure with NVIDIA for AI Large Language Model Training  \n",
      "Technical White Paper  \n",
      "   \n",
      "Model training or pretraining yields a  foundational LLM by training it on a large corpus of \n",
      "data. We vali dated our design to ensure the functionality of model training techniqu e \n",
      "available in the NeMo framework. Our goal in this validation was not to train a model to \n",
      "convergence  and generate a complete foundational model , but rather to train  for a defined \n",
      "numb er of steps in order to achieve the goals described here . \n",
      "The following list provides the details of our validation setup:  \n",
      "â€¢ Model architectures\n",
      " We trained primarily with 7B and 70B Llama 2 model  \n",
      "architectures . We also trained with  GPT model  architectures . \n",
      "â€¢ Foundation model pre-training using NeMo Framework\n",
      " See the NeMo \n",
      "documentation  for available playbooks.   \n",
      "â€¢ Cluster configuration\n",
      " We used a Slurm for cluster  management and job \n",
      "scheduling . \n",
      "â€¢ Dataset\n",
      " We used Pile datasets  for this validation . The Pile is an 825 GiB \n",
      "diverse, open source language modelling data set that consists of 22 smaller, \n",
      "high-quality datasets c ombined together , derived primarily from academic or \n",
      "professional sources . \n",
      "â€¢ Time for training\n",
      " Usually, data scientists train a model until it reaches \n",
      "convergence, a point influenced by factors like the dataset, model complexity, \n",
      "and chosen hyperparameters. Our aim was not to achieve convergence for \n",
      "every scenario, as it is specific to our chosen dataset and parameters, offering \n",
      "limited insight into a customers â€™ needs. To maintain a consistent metric across \n",
      "all scenarios, we conducted training jobs for a mini mum of 50 0 steps.  \n",
      "Model architecture selectio n \n",
      "Among the various LLMs available,  we selected  the 7B and 70B parameters of Llama 2  \n",
      "architectures  to use for  training , based on several key fa ctors:  \n",
      "â€¢ Resource use: Using these two models sizes helped us better understand the \n",
      "infrastructure resource usage and requirements for various training workload s \n",
      "for a range of model sizes.  \n",
      "â€¢ Ease of use: The models have been readily available for consumption, along \n",
      "with recipes and cookbook implementation s, makin g modification to the \n",
      "codebase easier for customersâ€™ use cases.  \n",
      "Para llelism  \n",
      "Below are the tensor and pipeline parallelism values we used for the model s. Tensor \n",
      "parallelism  and pipeline parallelism are generated by NeMo Megatron launcher based on \n",
      "model parameter size and number of GPUs.  \n",
      "Table 3.  Parallelism for Llama 2 architecture for training  \n",
      "Model  Configuration  \n",
      "Llama 2 7B  Tensor Parallelism = 2 \n",
      "Pipeline Parallelism = 1 \n",
      "Micro batch size = 1  \n",
      "Global batch size = 144 \n",
      "Sequence length = 4096  Validation \n",
      "results\n"
     ]
    }
   ],
   "source": [
    "import gradio as gr\n",
    "\n",
    "query_engine = Custom_Query_Engine()\n",
    "\n",
    "css = \"\"\"\n",
    ".app-interface {\n",
    "    height:80vh;\n",
    "}\n",
    ".chat-interface {\n",
    "    height: 75vh;\n",
    "}\n",
    ".file-interface {\n",
    "    height: 40vh;\n",
    "}\n",
    "\"\"\"\n",
    "with gr.Blocks(css=css) as demo:\n",
    "    gr.Markdown(\n",
    "    \"\"\"\n",
    "    <h1 style=\"text-align: center;\">NIMs Document Chatbot ðŸ’»ðŸ“‘âœ¨</h3>\n",
    "    \"\"\")\n",
    "    with gr.Row(equal_height=False, elem_classes=[\"app-interface\"]):\n",
    "        with gr.Column(scale=4, elem_classes=[\"chat-interface\"]):\n",
    "            test = gr.ChatInterface(fn=stream_response)\n",
    "        with gr.Column(scale=1):\n",
    "            url_input = gr.Textbox(label=\"Reference File URL\", lines=1)\n",
    "            scrape_button = gr.Button(\"Scrape Site\")\n",
    "            scrape_button.click(fn=vectorize_scrape, inputs=url_input, outputs=url_input)\n",
    "            # file_input = gr.File(elem_classes=[\"file-interface\"], file_types=[\"pdf\", \"csv\", \"text\", \"html\"], file_count=\"multiple\")\n",
    "            file_input = gr.File(elem_classes=[\"file-interface\"], file_types=[\"file\"], file_count=\"multiple\")\n",
    "            vectorize_button = gr.Button(\"Vectorize Files\")\n",
    "            vectorize_button.click(fn=vectorize_uploads, inputs=file_input, outputs=file_input)\n",
    "            use_rag = gr.Checkbox(label=\"Use Knowledge Base\")\n",
    "            use_rag.select(fn=toggle_knowledge_base, inputs=use_rag)\n",
    "            \n",
    "\n",
    "demo.launch(server_name=\"0.0.0.0\", ssl_verify=False, inline=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d53f0a9-91f9-4b5b-8792-bee9dc302ff7",
   "metadata": {},
   "source": [
    "**NOTE: To access the On-Prem NIM RAG Demo, please dont use the above URL, use this instead: `http://<YOUR-VM-IP-ADDRESS>:7860`**\\\n",
    "The gradio frontend for this specific demo is served on port `7860` \\\n",
    "For example, if my VM IP address is 172.27.193.230, go to  `http://172.27.193.230:7860`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
