{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e13e2b76-7b6f-451c-bbc0-ef777279dee8",
   "metadata": {},
   "source": [
    "## Install Dependencies\n",
    "We will be mainly interacting with HuggingFace, NVIDIA, and LlamaIndex. \\ \n",
    "Lets get started by installing the python dependencies needed for this demo. \\\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "588a45a5-c07c-4df3-9470-9ad05be77067",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%pip install gradio\n",
    "%pip install llama-index-llms-huggingface\n",
    "%pip install llama-index-llms-huggingface-api\n",
    "%pip install llama-index-embeddings-huggingface\n",
    "%pip install llama-index-embeddings-huggingface-api\n",
    "%pip install --upgrade --quiet llama-index-llms-nvidia llama-index-embeddings-nvidia llama-index-readers-file llama-index llama-index-readers-web"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be627c0e-aba3-4869-9421-73b2b027c942",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install \"transformers[torch]\" \"huggingface_hub[inference]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8085cd3f-6eb8-48d0-8ac8-b820e2ae98a7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install llama-index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c39609f-1020-4c30-96af-6fabd174941f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "import os\n",
    "os.environ[\"HF_TOKEN\"] = \"\"\n",
    "os.environ[\"NVIDIA_API_KEY\"] = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ede58d19-2346-406d-b9b0-3f3f49eed854",
   "metadata": {},
   "source": [
    "## Prepare Environment\n",
    "Lets prepare and set up the environment we need for interacting with various APIs. \\\n",
    "We will need the following API Keys: \\\n",
    "**HuggingFace Access Token** - to retrieve and download the gated Llama-3.1-8B-Instruct model (appy for access [here](https://huggingface.co/meta-llama/Llama-3.1-8B-Instruct))\\\n",
    "**NGC Key** - to download NIMs containers to run On-Prem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "60db213d-9b45-4d82-8313-9ea94dafaee5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "HF TOKEN (starts with hf_):  Â·Â·Â·Â·Â·Â·Â·Â·\n"
     ]
    }
   ],
   "source": [
    "hf_token = \"\"\n",
    "\n",
    "import getpass\n",
    "import os\n",
    "\n",
    "# del os.environ['HF_TOKEN']  ## delete key and reset\n",
    "if os.environ.get(\"HF_TOKEN\", \"\").startswith(\"hf_\"):\n",
    "    print(\"Valid HF_TOKEN already in environment. Delete to reset\")\n",
    "else:\n",
    "    hf_token = getpass.getpass(\"HF TOKEN (starts with hf_): \")\n",
    "    assert hf_token.startswith(\n",
    "        \"hf_\"\n",
    "    ), f\"{hf_token[:5]}... is not a valid key\"\n",
    "    os.environ[\"HF_TOKEN\"] = hf_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8c811987-388e-4c73-902c-e2bede9cfa9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "NVAPI Key (starts with nvapi-):  Â·Â·Â·Â·Â·Â·Â·Â·\n"
     ]
    }
   ],
   "source": [
    "import getpass\n",
    "import os\n",
    "\n",
    "# del os.environ['NVIDIA_API_KEY']  ## delete key and reset\n",
    "if os.environ.get(\"NVIDIA_API_KEY\", \"\").startswith(\"nvapi-\"):\n",
    "    print(\"Valid NVIDIA_API_KEY already in environment. Delete to reset\")\n",
    "else:\n",
    "    nvapi_key = getpass.getpass(\"NVAPI Key (starts with nvapi-): \")\n",
    "    assert nvapi_key.startswith(\n",
    "        \"nvapi-\"\n",
    "    ), f\"{nvapi_key[:5]}... is not a valid key\"\n",
    "    os.environ[\"NVIDIA_API_KEY\"] = nvapi_key"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2fbc377-fd2d-469a-a672-76c1d06aa22b",
   "metadata": {},
   "source": [
    "## Inferencing with On-Prem NVIDIA Inference Microservices (NIM)\n",
    "In this section, running the following code cells will consume a locally-deployed Llama-3.1-8B-Instruct NIM container and serve them for inferncing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d89bbb15-8e2d-4572-b392-5e8ebe7449d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# llama-parse is async-first, running the async code in a notebook requires the use of nest_asyncio\n",
    "import nest_asyncio\n",
    "\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dec04231-5ab0-4ac2-bf35-d767580a3fbd",
   "metadata": {},
   "source": [
    "### Sample inferencing with On-Prem NIM model\n",
    "In the following code cell, we will first specify the NIM endpoint serving Llama-3.1-8B-Instruct model, and execute a sample inference afterwards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "49eb25eb-7fb9-40d2-8fdb-82fd02b4c685",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dell Technologies is a multinational computer technology company that designs, develops, manufactures, markets, and supports computers and other electronic devices. The company was founded in 1984 by Michael Dell and is headquartered in Round Rock, Texas, USA.\n",
      "\n",
      "Dell Technologies is a leading provider of a wide range of products and services, including:\n",
      "\n",
      "1. **Personal computers**: Desktops, laptops, tablets, and mobile devices.\n",
      "2. **Servers**: Data center servers, storage systems, and networking equipment.\n",
      "3. **Storage**: Storage solutions, including hard disk drives, solid-state drives, and storage arrays.\n",
      "4. **Networking**: Networking equipment, including switches, routers, and wireless access points.\n",
      "5. **Virtualization**: Virtualization software and services, including VMware.\n",
      "6. **Cloud computing**: Cloud infrastructure, platform, and software as a service (IaaS, PaaS, SaaS).\n",
      "7. **Cybersecurity**: Cybersecurity solutions, including threat detection, incident response, and security consulting.\n",
      "8. **Services**: IT consulting, deployment, and support services.\n",
      "\n",
      "Dell Technologies is a global company with operations in over 180 countries and a workforce of over 130,000 employees. The company has a strong presence in the enterprise market, serving large businesses, governments, and educational institutions.\n",
      "\n",
      "In 2016, Dell acquired EMC Corporation, a leading provider of data storage and management solutions, in a $67 billion deal. This acquisition expanded Dell's offerings in the data center and cloud computing spaces.\n",
      "\n",
      "Today, Dell Technologies is a leading player in the technology industry, with a diverse portfolio of products and services that help organizations of all sizes to innovate, transform, and succeed in a rapidly changing world."
     ]
    }
   ],
   "source": [
    "from llama_index.llms.nvidia import NVIDIA\n",
    "from llama_index.core.llms import ChatMessage\n",
    "\n",
    "# connect to an chat NIM running at localhost:8000, spcecifying a specific model\n",
    "local_nim_llm = NVIDIA(\n",
    "    base_url=\"http://localhost:8000/v1\", model=\"meta/llama-3.1-8b-instruct\"\n",
    ")\n",
    "\n",
    "# content = \"\"\n",
    "# for completion in local_nim_llm.stream_complete(\"What is Dell Technologies?\"):\n",
    "#     content += completion.delta\n",
    "#     print(completion.delta, end=\"\")\n",
    "\n",
    "messages = [\n",
    "    ChatMessage(role=\"system\", content=\"You are a helpful assistant.\"),\n",
    "    ChatMessage(role=\"user\", content=\"What is Dell Technologies?\"),\n",
    "]\n",
    "\n",
    "content = \"\"\n",
    "for completion in local_nim_llm.stream_chat(messages):\n",
    "    content += completion.delta\n",
    "    print(completion.delta, end=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26c7acf1-6806-4b39-9c9e-30573b12359d",
   "metadata": {},
   "source": [
    "## Retrieval Augmented Generation (RAG) with On-Prem NIM\n",
    "In this section, we will code up RAG utilizing on-prem NIM, and eventually present those in a interactive chatbot where users can:\\\n",
    "1. scrape websites for additional content\n",
    "2. upload files for additional content\n",
    "3. Choose between utilizing knowledge base or not"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f94d6c45-4d49-4736-b287-98dc58f8f201",
   "metadata": {},
   "source": [
    "### Writing the logic for RAG\n",
    "In the following code dell, we will develop a custom LlamaIndex Query Engine Class which does the following:\n",
    "- Sets the deployed NIM endpoint for LLM inferencing\n",
    "- Loads and sets a local embedding model from HuggingFace \n",
    "- Instantiates a in-memory Vector Database\n",
    "- Instantiates and connects the LLM, Embedding Model, and Vector Databases together into a Query Engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4fff8467-21bb-4104-a651-8f2b7739875a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader, Settings\n",
    "from llama_index.readers.web import SimpleWebPageReader\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "from llama_index.llms.nvidia import NVIDIA\n",
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "os.environ['GRADIO_TEMP_DIR'] = \"/home/jovyan/work/gradio\"\n",
    "\n",
    "RAG_UPLOAD_FOLDER = \"/home/jovyan/work/rag-documents/\"\n",
    "\n",
    "\n",
    "class Custom_Query_Engine():\n",
    "    def __init__(self):\n",
    "        # self.SYSTEM_PROMPT = \"\"\"You are an AI assistant that answers questions in a friendly manner. Here are some rules you always follow:\n",
    "        # - Generate human readable output, avoid creating output with gibberish text.\n",
    "        # - Make use of the additional context given to provide better answers.\n",
    "        # - Elaborate on your responses based on the context given.\n",
    "        # - Give as much detail as you can to help the user with the query.\n",
    "        # \"\"\"\n",
    "        \n",
    "        self.llm = NVIDIA(\n",
    "            base_url=\"http://localhost:8000/v1\", model=\"meta/llama-3.1-8b-instruct\"\n",
    "        )\n",
    "\n",
    "        # self.embed_model = IpexLLMEmbedding(model_name=\"/llm-models/hf-models/bge-small-en-v1.5\", trust_remote_code=True)\n",
    "        self.embed_model = HuggingFaceEmbedding(model_name=\"BAAI/bge-small-en-v1.5\")\n",
    "            \n",
    "        Settings.llm = self.llm\n",
    "        Settings.embed_model = self.embed_model\n",
    "        \n",
    "        Path(RAG_UPLOAD_FOLDER).mkdir(parents=True, exist_ok=True)\n",
    "        self.use_rag = False\n",
    "\n",
    "    def toggle_rag(self, toggle):\n",
    "        self.use_rag = toggle\n",
    "        return self.use_rag\n",
    "\n",
    "    def get_rag_toggle(self):\n",
    "        return self.use_rag\n",
    "\n",
    "    def query(self, message):\n",
    "        return self.query_engine.query(message)\n",
    "\n",
    "    def query_without_rag(self, message):\n",
    "        messages = [\n",
    "            ChatMessage(role=\"system\", content=\"You are a helpful assistant.\"),\n",
    "            ChatMessage(role=\"user\", content=message),\n",
    "        ]\n",
    "        return self.llm.stream_chat(messages)\n",
    "\n",
    "    def reload_scraped(self, documents):\n",
    "\n",
    "        try:\n",
    "            del self.query_engine\n",
    "        except:\n",
    "            print(\"instantiating new query engine\")\n",
    "        else:\n",
    "            print(\"re-creating query engine\")\n",
    "\n",
    "        try:\n",
    "            del self.index\n",
    "        except:\n",
    "            print(\"instantiating new index\")\n",
    "        else:\n",
    "            print(\"re-creating index\")\n",
    "            \n",
    "        self.index = VectorStoreIndex.from_documents(documents, show_progress=True)\n",
    "        self.query_engine = self.index.as_query_engine(streaming=True, similarity_top_k=10)\n",
    "\n",
    "    def reload_uploaded(self, path):\n",
    "\n",
    "        try:\n",
    "            del self.query_engine\n",
    "        except:\n",
    "            print(\"instantiating new query engine\")\n",
    "        else:\n",
    "            print(\"re-creating query engine\")\n",
    "\n",
    "        try:\n",
    "            del self.index\n",
    "        except:\n",
    "            print(\"instantiating new index\")\n",
    "        else:\n",
    "            print(\"re-creating index\")\n",
    "            \n",
    "        self.documents = SimpleDirectoryReader(RAG_UPLOAD_FOLDER).load_data()\n",
    "        self.index = VectorStoreIndex.from_documents(self.documents, show_progress=True)\n",
    "        self.query_engine = self.index.as_query_engine(streaming=True, similarity_top_k=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b557b8d3-06a1-45fd-ab29-e9071d3a4ccb",
   "metadata": {},
   "source": [
    "### Writing additional helper functions for Gradio RAG Chatbot\n",
    "Next, we will write some additional helper functions to take in information passed via our Gradio Frontend, process these information and feed it into our query engine. These functions include:\n",
    "- Toggling of knowledge base\n",
    "- Scraping and vectorizing files grabbed from custom URLs\n",
    "- Vectorizig files uploaded by the user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e3f855e8-6b5f-432e-ae31-04d7b7dab2b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "import shutil\n",
    "import glob\n",
    "import os\n",
    "\n",
    "def stream_response(message, history):\n",
    "    print(f\"current RAG toggle is {query_engine.get_rag_toggle()}\")\n",
    "    if query_engine.get_rag_toggle():\n",
    "        print('using RAG')\n",
    "        response = query_engine.query(message)\n",
    "        print(response.source_nodes[0].get_content())\n",
    "        res = \"\"\n",
    "        for token in response.response_gen:\n",
    "            # print(token, end=\"\")\n",
    "            res = str(res) + str(token)\n",
    "            yield res\n",
    "    else:\n",
    "        print('not using RAG')\n",
    "        response = query_engine.query_without_rag(message)\n",
    "        res = \"\"\n",
    "        for token in response:\n",
    "            # print(token, end=\"\")\n",
    "            res = str(res) + str(token.delta)\n",
    "            yield res\n",
    "\n",
    "def vectorize_scrape(url, progress=gr.Progress()):\n",
    "    Path(RAG_UPLOAD_FOLDER).mkdir(parents=True, exist_ok=True)\n",
    "    UPLOAD_FOLDER = RAG_UPLOAD_FOLDER\n",
    "\n",
    "    prev_files = glob.glob(f\"{UPLOAD_FOLDER}*\")\n",
    "    for f in prev_files:\n",
    "        os.remove(f)\n",
    "\n",
    "    if not url:\n",
    "        return []\n",
    "    \n",
    "    documents = SimpleWebPageReader(html_to_text=True).load_data([url])\n",
    "\n",
    "\n",
    "    query_engine.reload_scraped(documents)\n",
    "    \n",
    "    return url\n",
    "\n",
    "def vectorize_uploads(files, progress=gr.Progress()):\n",
    "    Path(RAG_UPLOAD_FOLDER).mkdir(parents=True, exist_ok=True)\n",
    "    UPLOAD_FOLDER = RAG_UPLOAD_FOLDER\n",
    "\n",
    "    prev_files = glob.glob(f\"{UPLOAD_FOLDER}*\")\n",
    "    for f in prev_files:\n",
    "        os.remove(f)\n",
    "\n",
    "    if not files:\n",
    "        return []\n",
    "    \n",
    "    file_paths = [file.name for file in files]\n",
    "\n",
    "    for file in files:\n",
    "        shutil.copy(file.name, UPLOAD_FOLDER)\n",
    "\n",
    "    query_engine.reload_uploaded(UPLOAD_FOLDER)\n",
    "    \n",
    "    return file_paths\n",
    "\n",
    "def toggle_knowledge_base(use_rag):\n",
    "    print(f\"toggling use knowledge base to {use_rag}\")\n",
    "    query_engine.toggle_rag(use_rag)\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55a9908c-141b-40d0-bd11-eedef6c74a62",
   "metadata": {},
   "source": [
    "### Creating a Gradio UI for interacting with On-Prem NIM\n",
    "We will now create a Gradio frontend to consume the deployed On-Prem NIM model. \\\n",
    "Launching Gradio clients will consume ports incrementally starting from port `7860` \\\n",
    "As this is the second demo we are launching concurrently, it will be served on port `7860`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1f90e04a-ed23-4387-aea8-12ea2a564dd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.12/site-packages/gradio/components/chatbot.py:229: UserWarning: The 'tuples' format for chatbot messages is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style 'role' and 'content' keys.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://0.0.0.0:7860\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "toggling use knowledge base to True\n",
      "instantiating new query engine\n",
      "instantiating new index\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a98b11156f54a13ac83d294c3387d5c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Parsing nodes:   0%|          | 0/45 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "90f04daa733346fe9d0c6f429147c00a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating embeddings:   0%|          | 0/45 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current RAG toggle is True\n",
      "using RAG\n",
      "Chapter 2: AI Model Inferencing  \n",
      " \n",
      "10 Generative AI in the Enterprise - Inferencing  \n",
      "A Scalable and Modular Production Infrastructure with NVIDIA for Artificial Intelligence Large Language Model \n",
      "Inferencing  \n",
      "Design Guide  Challenges with inferencing  \n",
      "Inferencing, the process of using a trained model to generate predictions or responses, \n",
      "can come with several challenges. Common challe nges associated with inferencing in AI \n",
      "include:  \n",
      "â€¢ Computational resources â€”Inferencing can be computationally intensive, \n",
      "especially for large and complex models. Even though inferencing can be less \n",
      "demanding than model training or fine- tuning, generating predictions or responses \n",
      "in real time might require significant processing power, memory, and efficient use of \n",
      "hardware resources.  \n",
      "â€¢ Latency and responsivenessâ€” Achieving low -latency and highly responsive \n",
      "inferencing is crucial in many real -time applications. Bal ancing the computational \n",
      "demands of the model with the need for fast responses can be challenging, particularly when dealing with high volumes of concurrent user requests.  \n",
      "â€¢ Model size and efficiency â€”LLMs , such as GPT -3, can have millions or even \n",
      "billions of parameters. Deploying and running such models efficiently, particularly on resource- constrained devices or in edge computing scenarios, can be a \n",
      "challenge due to memory and storage requirements.  \n",
      "â€¢ Deployment scalabilityâ€” Scaling up the deployment of a model handles  \n",
      "increasing user demand. Ensuring that the system can handle concurrent \n",
      "inferencing requests and dynamically allocate resources to meet the workload can \n",
      "be complex, requiring careful architecture design and optimization.  \n",
      "â€¢ Model optimizatio n and compression â€”Optimizing and compressing models for \n",
      "inferencing is necessary to reduce memory and computational requirements, enabling efficient deployment on various devices or platforms. Balancing the trade -\n",
      "off between model size, inference speed, and accuracy is a nontrivial task.  \n",
      "â€¢ Explainability and interpretability â€”Understanding and explaining the reasoning \n",
      "behind the model's predictions or responses is crucial in many applications, particularly in domains where accountability, transparency, and eth ical \n",
      "considerations are of paramount importance. Ensuring the interpretability of the model's decisions during inferencing can be a challenge, especially for complex \n",
      "models like deep neural networks.  \n",
      "â€¢ Quality control and error handling â€”Detecting and handlin g errors or \n",
      "inaccuracies during inferencing is important to maintain the quality and reliability of the system. Implementing effective error handling, monitoring, and quality control \n",
      "mechanisms to identify and rectify issues is essential.  \n",
      "These challenges highlight the need for careful consideration and optimization in various aspects of inferencing, ranging from computational efficiency and scalability to model \n",
      "optimization, interpretability, and quality control. Addressing these challenges contributes to the development of robust and reliable AI systems for inferencing.  \n",
      "Dell Technologies and NVIDIA help solve these challenges by collaborating to deliver a \n",
      "validated and integrated hardware and software solution, built on Dell high-performance \n",
      "best-in-class infrastructure, and using the award- winning software stack and the industry -\n",
      "leading accelerator technology and AI enterprise software stack of NVIDIA.\n"
     ]
    }
   ],
   "source": [
    "import gradio as gr\n",
    "\n",
    "query_engine = Custom_Query_Engine()\n",
    "\n",
    "css = \"\"\"\n",
    ".app-interface {\n",
    "    height:80vh;\n",
    "}\n",
    ".chat-interface {\n",
    "    height: 75vh;\n",
    "}\n",
    ".file-interface {\n",
    "    height: 40vh;\n",
    "}\n",
    "\"\"\"\n",
    "with gr.Blocks(css=css) as demo:\n",
    "    gr.Markdown(\n",
    "    \"\"\"\n",
    "    <h1 style=\"text-align: center;\">NIMs Document Chatbot ðŸ’»ðŸ“‘âœ¨</h3>\n",
    "    \"\"\")\n",
    "    with gr.Row(equal_height=False, elem_classes=[\"app-interface\"]):\n",
    "        with gr.Column(scale=4, elem_classes=[\"chat-interface\"]):\n",
    "            example_prompts = [\n",
    "                \"Tell me a story.\",\n",
    "                \"What is Dell AI Factory? Please elaborate in detail.\",\n",
    "                \"What Infrastructure is used for AI inferencing? Describe these infrastructures and their purpose.\",\n",
    "                \"What Infrastructure is used for AI training? Describe these infrastructures and their purpose.\"\n",
    "            ]\n",
    "            test = gr.ChatInterface(fn=stream_response, examples=example_prompts)\n",
    "        with gr.Column(scale=1):\n",
    "            url_input = gr.Textbox(label=\"Reference File URL\", lines=1)\n",
    "            scrape_button = gr.Button(\"Scrape Site\")\n",
    "            scrape_button.click(fn=vectorize_scrape, inputs=url_input, outputs=url_input)\n",
    "            file_input = gr.File(elem_classes=[\"file-interface\"], file_types=[\"file\"], file_count=\"multiple\")\n",
    "            vectorize_button = gr.Button(\"Vectorize Files\")\n",
    "            vectorize_button.click(fn=vectorize_uploads, inputs=file_input, outputs=file_input)\n",
    "            use_rag = gr.Checkbox(label=\"Use Knowledge Base\")\n",
    "            use_rag.select(fn=toggle_knowledge_base, inputs=use_rag)\n",
    "            \n",
    "\n",
    "demo.launch(server_name=\"0.0.0.0\", ssl_verify=False, inline=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d53f0a9-91f9-4b5b-8792-bee9dc302ff7",
   "metadata": {},
   "source": [
    "**NOTE: To access the On-Prem NIM RAG Demo, please dont use the above URL, use this instead: `http://<YOUR-VM-IP-ADDRESS>:7860`**\\\n",
    "The gradio frontend for this specific demo is served on port `7860` \\\n",
    "For example, if my VM IP address is 172.27.193.230, go to  `http://172.27.193.230:7860`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
